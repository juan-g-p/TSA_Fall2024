
```{r}
library(fpp3)
library(patchwork)
```

# References

[1] Fama, Eugene (1970). "Efficient Capital Markets: A Review of Theory and Empirical Work". Journal of Finance. 25 (2): 383–417. doi:10.2307/2325486. JSTOR 2325486.

[2] Investopedia - Efficient Market Hypothesis [Link](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp)

# The Efficient Market Hypothesis and random walks

The goal of this assignment is to compare a prediction strategy based on the Efficient Market Hypothesis (EMH) with some of the first non-trivial models you have learnt: simple exponential smoothing and trended exponential smoothing (additive and additive damped versions). To have an additional reference point, we will also include an auto-arima model, for which you will be provided with the syntax. The details of these models will be explained in the second part of this subject.

Essentially, the EMH states that, in a perfectly efficient market, stocks are accurately priced and reflect all available information. If it holds true, it implies that it impossible to consistently beat the market by analysis.

In his influential 1970 paper ([1]), Eugene Fama proposes three forms of the EMH that differ in the information considered reflected on the prices:

1. *Weak-form of the EMH*: considers information contained in historical prices.
2. *Semi-strong form of EMH*: considers information publicly available beyond historical prices.
3. *Strong-form*: considers private (privileged) information.

Here we are going to consider the weak-form of the EMH and work only with the information contained in historical prices. If the EMH holds, what we are saying in math terms is that the price of a stock asset follows a random walk. A random walk is a time series process in which the value at time t is the value at t-1 plus a random shock of uncorrelated white noise:

$$
y_t = y_{t-1} + \varepsilon_t
$$
In this situation, the difference between the price at t and the price at t-1 is simply a random shock.

$$
y_t - y_{t-1} =  \varepsilon_t
$$

This corresponds with the Naive forecasting method, which defined the forecast at time T+h (beyond the training data) as the value at time T (the last point for which data is available).

$$
\hat{y}_{T+h|T} = y_T 
$$

If we set h=1 in the equation above and then make the change of variable t=T+1 (see theory session on benchmark models), we reach the equation for the fitted values of the Naive model:

$$
\hat{y}_{t|t-1} = y_{t-1} \ \ \ \ \ \text{(fitted values of the Naive model)}
$$
The residuals are, by definition, the value of the time series minus the fitted values. Therefore, replacing the formula obtained above, for the naive model the value of the residuals at a generic point in time t is:

$$
y_t - \hat{y}_{t|t-1} = y_t - y_{t-1}
$$
If the process is a random walk, these residuals should be uncorrelated white noise. If we consider the equation for the random walk, we may see this:

$$
y_t = y_{t-1} + \varepsilon_t \rightarrow y_t - y_{t-1} = \varepsilon_t
$$
# Assignment

### **1. Load the historical data and create an index associated to the training day** (0 points)

```{r}
tr_data <- readr::read_csv(file.choose()) # Creates a window for you to select the .csv file
tr_data <- tr_data %>% 
           as_tsibble(key = symbol, index = 'trading_day') %>% 
           fill(close, .direction="downup") # Interpolate missing values
```

### **1.1 Create a time-plot of your data and briefly describe it (max 75 words)** (1 points)

```{r}
# YOUR CODE GOES HERE
# DO NOT CREATE ADDITIONAL CODE SNIPPETS
```

------

YOUR ANSWER GOES HERE. MAX 50 WORDS.

------

yaml

For this, complete the code below. In the end you will have a single tsibble where each training dataset is identified by `.id`.

NOTE: we do not use stretch-tsibble in this case because we only want to produce two trading datasets.

```{r}
nrow(df)
```


```{r}
n_obs_80 <- # Your code goes here. Compute the integer number corresponding 80% of the observations in the data
n_obs_60 <- # Your code goes here. Compute the integer number ccorresponding 60% of the observations in the data
n_obs_list <- c(n_obs_60, n_obs_80)
train_sets <- tibble() # EMPTY TIBBLE ON WHICH WE WILL INSERT THE DATA

# For loop produces the training datasets and joins them together in a single dataframe.
# Simply run the for loop
for (i in seq_along(n_obs_list)) {
  train_i <- tr_data %>% slice(1:n_obs_list[i]) %>% mutate(.id=i)
  train_sets <- bind_rows(train_sets, train_i)
}

# Turn into a tsibble for compatibility with the fable library.
# Simply run the code
train_sets <- as_tsibble(train_sets, key=.id, index=trading_day)
```

### **3. Fit the following models to each of the training datasets:** (1 points)

* a Naïve model. Call it `naive`.
* a Simple Exponential Smoothing model. Call it `ses`
* a Trended Exponential Smoothing model with and additive damped trend and no seasonality. Call it `holts_damped`
* an automatically selected ARIMA model. You still do not know about ARIMA models, so the formula for that is given below.

Name the object where you store the fitted objects `fit_train_sets`

```{r}
# YOUR CODE GOES HERE, DO NOT CREATE ADDITIONAL CODE SNIPPETS
# SIMPLY COMPLETE THE CODE BELOW WHERE REQUIRED
fit_train_sets <-
  train_sets %>% 
  model(
    naive = # Your code goes here
    ses = # Your code goes here
    holts_damped =  # Your code goes here
    autoarima = ARIMA(close)
  )

fit_train_sets
```

### **4. Generate forecasts of 5 trading days for each of the trained models** (1 point)

The result of this step should be a `fable` or forecast table. Name it `fc_train_sets`

Add a column `h` to `fc_train_sets` indicating the forecast horizon corresponding to each forecast. See the session on CV to learn how to do this.

After adding that column, apply the following command: `as_fable(response = "close", distribution = close)`. This is to ensure that the output after performing these operations is still a `fable`, which can be used in combination with the function `accuracy()`.

Do not forget the command `ungroup()` at the end of the process to ensure that the result is not grouped.

```{r}
fc_train_sets <- 
  fit_train_sets %>% 
  # YOUR CODE GOES HERE
```

### **5. Compute errors and error metrics for each training dataset

#### 5.1 Compute the absolute error and the absolute percent error (without averaging) at every point.

To do this, follow the steps below:

```{r}
# 0. Select only the Symbol, trading_day and close variables. Then rename the column "close"
# to close_data using rename(close_data = close). Store the result in aux1.
aux1 <- 
  tr_data %>% 
  ...

# 1. Use a left_join() between fc_train_sets (left table) and aux1 (right table)
# to incorporate the actual value of the variable "close" to fc_train_sets. 
# Store the result in a new variable called "df_errors".
df_errors <- 
  fc_train_sets %>% 
  left_join(aux1, ...)

# 2. Create two new columns in df_errors: 
    # abs_error: absolute error of each individual forecast.
    # abs_perc_error: percentage error of each individual forecast
df_errors <-
  df_errors %>% 
  
# 3. Check the result. It should have 40 rows and containing the abs_error
#    and the absolute percentage error associated to eachforecast.
#   (4 models * 2 training datasets * 5 forecasts per model)
df_errors 

```

#### 5.2 Produce the following graphs and answer the questions posed (2.5 points)

You are tasked with generating the following graphs and answering the questions at the end of this section.

##### TWO GRAPHS FOR THE ABSOLUTE ERRORS. ONE GRAPH FOR EACH TRAINING DATASET.

The graphs shall be generated using the library `ggplot`.

These graphs shall fulfill the following:

- x-axis corresponds to the forecast horizon and its grid shows only integer values (the values of h).
- y-axis corresponds to the values of the absolute error. **The y-axis on both graphs shall have the same scale to be able to compare among them**.
- In summary, each graph shall have 4 lines, one line for each of the four models fitted.

I recommend using `facet_wrap(".id")` as the last ggplot command to produce one graph for each trading dataset, but you are free to proceed otherwise if you reach the same result.

```{r}
df_errors %>% 
  # YOUR CODE GOES HERE
```

##### TWO GRAPHS FOR THE ABSOLUTE PERCENT ERRORS. ONE GRAPH FOR EACH TRAINING DATASET.

The graphs shall be generated using the library `ggplot`.

These graphs shall fulfill the following:

- x-axis corresponds to the forecast horizon and its grid shows only integer values (the values of h).
- y-axis corresponds to the values of the absolute percent error. **The y-axis on both graphs shall have the same scale to be able to compare among them**.
- In summary, each graph shall have 4 lines, one line for each of the four models fitted.

I recommend using `facet_wrap(".id")` as the last ggplot command to produce one graph for each trading dataset, but you are free to proceed otherwise if you reach the same result.

```{r}
df_errors %>% 
  # YOUR CODE GOES HERE
```

##### QUESTIONS ABOUT GRAPHS:

1. Does the training dataset have an effect on the errors?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

2. Does one of the models outperform the other? Are these results the same for both training datasets?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

#### 5.3 For each training dataset, generate the RMSE, MAE and MAPE of each model averaged over all forecast horizons.

To do this, use two methods:

##### **METHOD 1: use `accuracy(tr_data, by=...)` on fc_train_sets**. Store the result in `error_metrics_accuracy`

```{r}
error_metrics_accuracy <- 
  fc_train_sets %>% 
  accuracy(tr_data, ...) 
```

##### **METHOD 2: using the abs_error and abs_perc_error columns in `df_errors`, group_by appropriately and use summarize + the formulas for the error metrics**. Store the result in `error_metrics_manual`

```{r}
error_metrics_manual <- 
  df_errors %>% 
  as_tibble() %>% 
  group_by(...) %>% 
  ...
```

##### COMPARE METRICS: run the code below to compare metrics and check that both match exactly

```{r}
# COMPARE METRICS
all.equal(error_metrics_manual$MAE, error_metrics_accuracy$MAE)
all.equal(error_metrics_manual$RMSE, error_metrics_accuracy$RMSE)
all.equal(error_metrics_manual$MAPE, error_metrics_accuracy$MAPE)
```

### **6. Instead of running the process for two training datasets separately, do cross validation** (2.5 points)

#### 6.1 Take all the steps necessary to obtain the cross validated MAE, RMSE and MAPE for each combination of model and forecast horizon. 

- The final table shall therefore have 4 x 5 rows (4 models and 5 forecasts per model). For each row, a column for MAE, another for RMSE and another for MAPE.

- The smallest training dataset shall contain 60% of the trading days.
- The training datasets shall increase in steps of 5 trading days.

```{r}
# 1. Create training datasets
init_obs <- as.integer(nrow(tr_data)*0.6)

tr_data_cv <- 
  tr_data %>% 
  # YOUR CODE GOES HERE

# 2. Fit models to all training datasets
fit_cv <- 
  tr_data_cv %>% 
  # YOUR CODE GOES HERE 
  
# 3. Produce forecasts for each training dataset
    # and generate column h to segregate the accuracy metrics.
fc_cv <- 
  fit_cv %>% 
  # YOUR CODE GOES HERE

# Accuracy metrics for each model by forecast horizon:
summary_cv <- 
  fc_cv %>% 
    accuracy( # YOUR CODE GOES HERE) %>% 
    # YOUR CODE GOES HERE
```

#### 6.2 Obtain the cross-validated accuracy metrics manually (layout)

Obtain the same table you got in 6.1, but this time computing the errors yourself using their formulas. Store the result in `summary_cv_manual`

```{r}
# YOUR CODE GOES HERE
```

At the end, check that both calculations match:

```{r}
# Check (should evaluate to TRUE)
all.equal(summary_cv$MAE, summary_cv_manual$MAE)
all.equal(summary_cv$RMSE, summary_cv_manual$RMSE)
all.equal(summary_cv$MAPE, summary_cv_manual$MAPE)
```

### **7. Produce the following graphs and answer the questions** (2.5 points) 

Produce two graphs. One for the MAE and one for the MAPE. The graphs shall fulfill:

1. The x-axis is discrete and corresponds to the forecast horizon
2. The y-axis correspondds to the values of the error (MAE or MAPE depending on the graph)
3. One line per model, so in total we should have three lines in each graph.

- x-axis corresponds to the forecast horizon and its grid shows only integer values
- y-axis corresponds to the values of the MAE or the MAPE, depending on the graph.
- Each graph shall have 4 lines, one line for each of the four models fitted depicting the absolute errors.

```{r}
# YOUR CODE GOES HERE
```

* What is the difference between these graphs and the ones we generated in question 5.2? Answer briefly below.

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

* Does one of the models outperform the other?

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

### **8. Reflect on the results and present some conclusions** (0 point)

------

YOUR ANSWER GOES HERE.
MAX 100 WORDS.

------
