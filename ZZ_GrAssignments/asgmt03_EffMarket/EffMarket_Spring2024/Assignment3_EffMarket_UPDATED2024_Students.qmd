---
title: "Efficient Market Hypothesis (Group assignment)"
self-contained: true
self-contained-math: true
format: html
editor: source
params:
  solutions: false
  print_sol: false
toc: true
toc-location: left
toc-depth: 6
---

```{r}
library(fpp3)
library(patchwork)
```

# References

[1] Fama, Eugene (1970). "Efficient Capital Markets: A Review of Theory and Empirical Work". Journal of Finance. 25 (2): 383–417. doi:10.2307/2325486. JSTOR 2325486.

[2] Investopedia - Efficient Market Hypothesis [Link](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp)

# The Efficient Market Hypothesis

The goal of this assignment is to compare a prediction strategy based on the Efficiet Market Hypothesis (EMH) with some of the first non-trivial models you have learnt: simple exponential smoothing and trended exponential smoothing (additive and additive damped versions).

Essentially, the EMH states that stocks are accurately priced and reflect all available information. If it holds true, it implies that it impossible to consistently beat the market by analysis.

In his influential 1970 paper ([1]), Eugene Fama proposes three categories of efficiency that differ in the information considered reflected in the prices:

1. *Weak-form of the EMH*: considers information contained in historical prices.
2. *Semi-strong form of EMH*: considers information publicly available beyond historical prices.
3. *Strong-form*: considers private (privileged) information.

Here we are going to consider the weak-form of the EMH and work only with the information contained in historical prices.

# Assignment

The assignment consists in:

### **1. Load the historical data and create an index associated to the training day** (0 points)

As an additional resource, a notebook is provided to download stock market data.
  
The date ranges to be used are:

  * **Initial date**: 7-th of May 2017
  * **Final date**: the current date.
  
**Your final dataset must be an object of type `tsibble` with the following characteristics**:

* key = symbol
* index = trading_day

```{r}
tr_data <- readr::read_csv("FTSE_prices.csv")
tr_data <- tr_data %>% 
           as_tsibble(key = symbol, index = 'trading_day') %>% 
           fill(close, .direction="downup") # Interpolate missing values
```

### **1.1 Create a time-plot of your data and briefly describe it (max 75 words)** (1 points)

```{r}
# YOUR CODE GOES HERE
# DO NOT CREATE ADDITIONAL CODE SNIPPETS
```

------

YOUR ANSWER GOES HERE. MAX 50 WORDS.

------

### **2. Create two training datasets containing 80% and 60% of your data** (1 points)

For this, complete the code below. In the end you will have a single tsibble where each training dataset is identified by `.id`.

NOTE: we do not use stretch-tsibble in this case because we only want to produce two trading datasets.

```{r}
n_obs_80 <- as.integer(nrow(tr_data)*0.8)
n_obs_60 <- as.integer(nrow(tr_data)*0.6)
n_obs_list <- c(n_obs_60, n_obs_80)
train_sets <- tibble() # EMPTY TIBBLE

for (i in seq_along(n_obs_list)) {
  train_i <- tr_data %>% slice(1:n_obs_list[i]) %>% mutate(.id=i)
  train_sets <- bind_rows(train_sets, train_i)
}

train_sets <- as_tsibble(train_sets, key=.id, index=trading_day)
```

### **3. Fit the following models to each of the training datasets:** (1 points)

* a Naïve model (`naive`).
* a Simple Exponential Smoothing model. Call it `ses_tr`
* a Trended Exponential Smoothing model with and additive dampled trend and no seasonality. Call it `holts_damped`

Name the object where you store the fitted objects `fit_train_sets`

```{r}
# YOUR CODE GOES HERE, DO NOT CREATE ADDITIONAL CODE SNIPPETS
fit_train_sets <- train_sets %>% 
  model(
    naive = NAIVE(close),
    ses =  ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped = ETS(close ~ error("A") + trend("Ad") + season("N"))
  )

fit_train_sets
```

### **4. Generate forecasts of 10 trading days for each of the trained models** (1 point)

The result of this step should be a `fable` or forecast table. Name it `fc_train_sets`

Add a column `h` to `fc_train_sets` indicating the forecast horizon of each forecast.

* After adding that column, apply the following command: `as_fable(response = "close", distribution = close)`. This is to ensure that the output after performing these operations is still a `fable`, which can be used in combination with the function `accuracy()`

```{r}
fc_train_sets <- 
  fit_train_sets %>% 
  forecast(h=10) %>% 
  group_by(.id, .model) %>% 
  mutate(h=row_number()) %>% 
  ungroup() %>% 
  as_fable(response = "close", distribution = close) %>% 
  select(h, everything())
```

### **5. Compute the forecasts errors. Do this in two ways to fully understand the process:

Option 1: **Manually**.

```{r}
# 1. Use a left_join() with tr_data to incorporate the actual value of the 
# variable "close" to fc_train_sets. Store the result in "error_metrics".
# NOTE: since fc_train_sets already has a column called close, the result of the
# left_join() will have two columns: close.x and close.y to resolve that conflict.
error_metrics <- 
  fc_train_sets %>% 
  left_join(tr_data, by="trading_day")

# 2. Create two new columns: abs_error and abs_perc_error corresponding to the
# absolute error and the absolute percentage error for each combination of model
# training dataset and forecast horizon.
error_metrics <- 
  error_metrics %>% 
  mutate(
    abs_error = abs(close.y - .mean),
    abs_perc_error = abs((close.y - .mean)/close.y)*100
  )

# 3. Inspect result
error_metrics <- 
  error_metrics %>% 
  as_tibble() %>% 
  arrange(.model, .id, h)
```

Option 2: **Using the funciton accuracy** and telling it to compute errors for each combination of forecast horizon, training dataset and model.

After that answer: why are RMSE and MAE the same in this case?

```{r}
fc_train_sets %>% 
  accuracy(tr_data, by=c("h", ".id", ".model"))
```

----------

YOUR ANSWER GOES HERE

----------

### **6. Produce the following graphs and answer the questions** (2.5 points) 

Produce two graphs, one for the absolute errors and one for the absolute percentage errors, that fulfil:

1. The x-axis is discrete and corresponds to the forecast horizon
2. The y-axis correspondds to the values of the error (absolute or absolute percentage error)
3. Each graph has 6 curves, one for each combination of model and training dataset.

For this, I suggest using the 

```{r}
summary_abs_error <- 
  error_metrics %>% 
  unite(error_metrics %>% as_tibble(), c(".model"))
  # filter(.model == "naive_tr") %>% 
  ggplot(aes(x = h, y = abs_error, color = c(factor(.id), .model))) + # Coerce h to factor
  geom_point() +
  geom_line(alpha = 0.25) +
  scale_x_continuous(
    breaks = seq(1, max(error_metrics$h))
  ) + 
  ggtitle("abs errors naive model")

# PERC ERROR
summary_perc_error <-
  errors_manual %>% 
  filter(.model == "naive_tr") %>% 
  ggplot(aes(x = h, y = abs(perc_error), color = factor(.id))) + # Coerce h to factor
  geom_point() +
  geom_line(alpha = 0.25) +
  scale_x_continuous(
    breaks = seq(1, max(errors_manual$h))
  ) + 
  ggtitle("perc errors naive model")
```


Then answer these questions:

* Does the training dataset have an effect on the errors?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

* Does one of the models outperform the other?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

### **7. Evaluate the same metrics on the same models, but this time using cross validation** (2.5 points)

The end-goal is that you produce the same final graph (facet_wrap graph on the error metrics), but this time only three lines per graph should be depicted (one line per model).

The smallest training dataset shall contain 60% of the trading days. The trading datasets shall increase in steps of 2 trading days.

* What is the difference between these graphs and the ones we generated before? Answer briefly below.

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

* Does one of the models outperform the other?

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

### **8. Reflect on the results and present some conclusions** (0 point)

------

YOUR ANSWER GOES HERE.
MAX 100 WORDS.

------
