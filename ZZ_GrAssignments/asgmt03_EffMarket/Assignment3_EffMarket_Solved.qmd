---
title: "Efficient Market Hypothesis (Group assignment)"
self-contained: true
self-contained-math: true
format: html
editor: source
params:
  solutions: false
  print_sol: false
toc: true
toc-location: left
toc-depth: 6
---

```{r}
library(fpp3)
library(patchwork)
```

# References

[1] Fama, Eugene (1970). "Efficient Capital Markets: A Review of Theory and Empirical Work". Journal of Finance. 25 (2): 383–417. doi:10.2307/2325486. JSTOR 2325486.

[2] Investopedia - Efficient Market Hypothesis [Link](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp)

# The Efficient Market Hypothesis

The goal of this assignment is to compare a prediction strategy based on the Efficiet Market Hypothesis (EMH) with some of the first non-trivial models you have learnt: simple exponential smoothing and trended exponential smoothing (additive and additive damped versions).

Essentially, the EMH states that stocks are accurately priced and reflect all available information. If it holds true, it implies that it impossible to consistently beat the market by analysis.

In his influential 1970 paper ([1]), Eugene Fama proposes three categories of efficiency that differ in the information considered reflected in the prices:

1. *Weak-form of the EMH*: considers information contained in historical prices.
2. *Semi-strong form of EMH*: considers information publicly available beyond historical prices.
3. *Strong-form*: considers private (privileged) information.

Here we are going to consider the weak-form of the EMH and work only with the information contained in historical prices.

# Assignment

The assignment consists in:

### **1. Load the historical data and create an index associated to the training day** (0 points)

As an additional resource, a notebook is provided to download stock market data.
  
The date ranges to be used are:

  * **Initial date**: 7-th of May 2017
  * **Final date**: the current date.
  
**Your final dataset must be an object of type `tsibble` with the following characteristics**:

* key = symbol
* index = trading_day

```{r}
tr_data <- readr::read_csv("FTSE_prices.csv")
tr_data <- tr_data %>% 
           as_tsibble(key = symbol, index = 'trading_day') %>% 
           fill(close, .direction="downup") # Interpolate missing values
```
### **1.1 Create a time-plot of your data and briefly describe it (max 75 words)** (1 points)

```{r}
# YOUR CODE GOES HERE
# DO NOT CREATE ADDITIONAL CODE SNIPPETS
```

------

YOUR ANSWER GOES HERE. MAX 50 WORDS.

------

```{r, include=params$print_sol}
autoplot(tr_data)
```

```{r, include=params$print_sol, eval=FALSE}
The data shows a period of overall growth alternating growth and recession in the
price.

However there are difficulties associated with judging the overall trend of aggregated
indexes such as SP500 or FTSE100. Among others:
  
* Complexity and diversity of constituent stocks
* Changes in constituent stocks
* Economic cycles (for example, the data selected leaves out the COVID crisis)
* Geopolitical factors
* Market irrationality
* Lagging nature of indices (reflect what happened rather than what will happen)
* ...
```

### **2. Create two training datasets containing 80, 60 of your data** (1 points)

Call them `train_1` and `train_2`.

```{r}
# YOUR CODE GOES HERE
# DO NOT CREATE ADDITIONAL CODE SNIPPETS
```

```{r}
#SOLUTION
n_rows <- nrow(tr_data)
train_1 <- filter(tr_data, trading_day <= (n_rows * 0.8))
train_2 <- filter(tr_data, trading_day <= (n_rows * 0.6))
```

### **3. Fit the following models to each of the training datasets:** (1 points)

* a Naïve model (`naive`).
* a Simple Exponential Smoothing model. Call it `ses_tr`
* a Trended Exponential Smoothing model with and additive dampled trend and no seasonality. Call it `holts_damped`

```{r, eval=FALSE}
# YOUR CODE GOES HERE, DO NOT CREATE ADDITIONAL CODE SNIPPETS
fit_tr1 <- train_1 %>% 
  model(
    naive_tr1 = 
    ses_tr1 =  
    holts_damped_tr1 = 
  )

fit_tr2 <- train_2 %>% 
  model(
    naive_tr2 = 
    ses_tr2 =  
    holts_damped_tr2 = 
  )

# Bind all the models in a single mable to handle forecasts more easily.
fit <- bind_cols(
  fit_tr1 %>% select(-symbol), # Exclude column symbol to be able to bind columns
  fit_tr2 %>% select(-symbol)
)

fit
```



```{r}
#SOLUTION
fit_tr1 <- train_1 %>% 
  model(
    naive_tr1 = NAIVE(close),
    ses_tr1 =  ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped_tr1 = ETS(close ~ error("A") + trend("Ad") + season("N"))
  )

fit_tr2 <- train_2 %>% 
  model(
    naive_tr2 = NAIVE(close),
    ses_tr2 =  ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped_tr2 = ETS(close ~ error("A") + trend("Ad") + season("N"))
  )

# Bind all the models in a single mable to handle forecasts more easily.
fit <- bind_cols(
  fit_tr1 %>% select(-symbol), # Exclude column symbol to be able to bind columns
  fit_tr2 %>% select(-symbol)
)

fit
```

### **4. Generate forecasts of 10 trading days for each of the trained models** (1 point)

The result of this step should be a `fable` or forecast table. Name it `fc`

Add a column `h` to `fc` indicating the forecast horizon of each forecast.

* After adding that column, apply the following command: `as_fable(response = "close", distribution = close)`. This is to ensure that the output after performing these operations is still a `fable`, which can be used in combination with the function `accuracy()`

```{r}
#SOLUTION
fc <- fit %>% forecast(h=10)
fc

fc <- fc %>% 
  group_by(.model) %>%
  mutate(h = row_number()) %>%
  ungroup() %>%
  as_fable(response = "close", distribution = close)
```
### **5. For each combination of model, training dataset and forecast horizon compute the RMSE, MAE and MAPE. Store the result in a variable called `summary`** (1 point)

```{r, warning=FALSE, message=FALSE}
#SOLUTION
summary <- accuracy(fc, tr_data, by = c("h", ".model")) %>% select(-c(MPE, MASE, RMSSE, ACF1, ME))
summary
```

### **6. Produce the following graphs and answer the questions** (2.5 points) 

Using `facet_wrap` produce a graph for the RMSE and another one for the MAPE for each forecast 

The graphs should depict the RMSE for each forecast horizon and for each combination of model and training dataset.

```{r}
# YOUR CODE GOES HERE
# YOUR ANSWER GOES HERE
```

```{r}
summary %>%
  pivot_longer(c(RMSE, MAPE), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = h, y = value, color = .model)) + # Coerce h to factor
  geom_point() +
  geom_line(alpha = 0.25) +
  facet_wrap(~ metric, nrow = 2, scales = "free") + 
  scale_x_continuous(
    breaks = seq(1, max(summary$h))
  )
```

Then answer these questions:

* Does the training dataset have an effect on the errors?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

```{r, eval=FALSE}
From the graphs above we can observe that:

* For a given train-test split all the models are very close in performance. So
much so that their curves are overlaid.
* When the train-test split is changed, the performance of all models 
experiences a drastic change.

In conclusion, it does indeed have an impact
```


* Does one of the models outperform the other?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

```{r, eval=FALSE}
It may therefore be concluded that for the two models at hand and this 
particular data, the particular split performed on the data has a greater 
effect than the model chosen.
```

### **7. Evaluate the same metrics on the same models, but this time using cross validation** (2.5 points)

The end-goal is that you produce the same final graph (facet_wrap graph on the error metrics), but this time only three lines per graph should be depicted (one line per model).

The smallest training dataset shall contain 60% of the trading days. The trading datasets shall increase in steps of 2 trading days.

```{r}
# SOLUTION
tr_data_cv <- tr_data %>%
  stretch_tsibble(.init= as.integer(0.6*nrow(tr_data)), .step = 2)

fit_cv <- tr_data_cv %>%
  model(
    naive = NAIVE(close),
    ses =  ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped = ETS(close ~ error("A") + trend("Ad") + season("N"))
  )

fc_cv <- fit_cv %>% 
  forecast(h = 7) %>%
  group_by(.id, .model) %>%
  mutate(h = row_number()) %>%
  ungroup() %>%
  as_fable(response = "close", distribution = close)

summary_cv <- fc_cv %>%
  accuracy(tr_data, by=c("h", ".model")) %>% select(-c(MPE, MASE, RMSSE, ACF1, ME))

summary_cv %>% 
  pivot_longer(c(RMSE, MAPE), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = h, y = value, color = .model)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ metric, nrow = 2 , scales = "free")
```
```{r}

```

* What is the difference between these graphs and the ones we generated before? Answer briefly below.

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

```{r, eval=FALSE}
The main difference is that the graph below have been computed averaging over many
training datasets, making their results statistically more representative of what
would happen on average if we used these model to forecast this data

The previous graph corresponded to a single train test split, which is why the
results are more dependent on the particular train test split than on the model.
```

* Does one of the models outperform the other?

```{r, eval=FALSE}
The cross-validated results show that both models behave, on average over all
training datasets, virtually in the same manner.
```

### **8. Reflect on the results and present some conclusions** (0 point)

------

YOUR ANSWER GOES HERE.
MAX 100 WORDS.

------

**POSSIBLE ANSWER**

The strategy based on the NAIVE model follows the weak form of the EMH, that is, it considers that the current prices reflect all available information. The NAIVE model has 0 parameters, it is the simplest method we can fit and serves as a benchmark.

The other two models used introduce some complexity allowing them to weight observations exponentially and also to introduce a damped trend. However, the cross-validated accuracy metrics show that this increased complexity does not seem to provide any substantial improvement.

Regarding the accuracy metrics, the RMSE signals an error of up to 150 dollars for the forecast horizons considered. In realtive terms, this error is around 1.6 percent. as singaled by the MAPE. Is our model sufficiently good? Well the answer it that **it depends on the purpose of the model.**

* If the goal of the model is to do trading, of course the model is not sufficiently good. It also does not include any information beyond the price.
* If the goal of the model is to build an impression about the level of the market for a forecast horizon of 10 with an uncertainty quantified by the corresponding prediction intervals, the model allows us to understand the uncertainty associated with the NAIVE the SES and the Holt's damped forecast and what level of change we may see with a forecast horizon of 10 days.

**END OF POSSIBLE ANSWER**

