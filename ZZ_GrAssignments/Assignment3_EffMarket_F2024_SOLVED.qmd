---
title: "Efficient Market Hypothesis (Group assignment)"
self-contained: true
self-contained-math: true
format: html
editor: source
params:
  solutions: false
  print_sol: false
toc: true
toc-location: left
toc-depth: 6
---

```{r}
library(fpp3)
library(patchwork)
```

# References

[1] Fama, Eugene (1970). "Efficient Capital Markets: A Review of Theory and Empirical Work". Journal of Finance. 25 (2): 383–417. doi:10.2307/2325486. JSTOR 2325486.

[2] Investopedia - Efficient Market Hypothesis [Link](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp)

# The Efficient Market Hypothesis and random walks

The goal of this assignment is to compare a prediction strategy based on the Efficinet Market Hypothesis (EMH) with some of the first non-trivial models you have learnt: simple exponential smoothing and trended exponential smoothing (additive and additive damped versions). To have an additional reference, we will also include an auto-arima model, for which you will be provided with the syntax.

Essentially, the EMH states that stocks are accurately priced and reflect all available information. If it holds true, it implies that it impossible to consistently beat the market by analysis.

In his influential 1970 paper ([1]), Eugene Fama proposes three categories of efficiency that differ in the information considered reflected on the prices:

1. *Weak-form of the EMH*: considers information contained in historical prices.
2. *Semi-strong form of EMH*: considers information publicly available beyond historical prices.
3. *Strong-form*: considers private (privileged) information.

Here we are going to consider the weak-form of the EMH and work only with the information contained in historical prices. If this hypothesis holds, what we are saying in math terms is that the price of a stock market follows a random walk. A random walk is a time series process in which the value at time t is the value at t-1 plus a random shock of uncorrelated white noise:

$$
y_t = y_{t-1} + \varepsilon_t
$$
In this situation, the difference between the price at t and the price at t-1 is simply a random shock.

This corresponds with the Naive forecasting method, which defined the forecast at time T+h (beyond the training data) as the value at time T (the last point for which data is available).

$$
\hat{y}_{T+h|T} = y_T 
$$

If we set h=1 in the equation above and then make the change of variable t=T+1 (see theory session on benchmar models), we reach the equation for the fitted values of the Naive model:

$$
\hat{y}_{t|t-1} = y_{t-1} \ \ \ \ \ \text{(fitted values of the Naive model)}
$$
The residuals are, by definition, the value of the time series minus the fited values. Therefore, replacing the formula obtained above, for the naive model the value of the residuals at a generic point in time t is:

$$
y_t - \hat{y}_{t|t-1} = y_t - y_{t-1}
$$
If the process is a random walk, these residuals should be uncorrelated white noise. If we consider the equation for the random walk, we may see this:

$$
y_t = y_{t-1} + \varepsilon_t \rightarrow y_t - y_{t-1} = \varepsilon_t
$$
# Assignment

### **1. Load the historical data and create an index associated to the training day** (0 points)

```{r}
tr_data <- readr::read_csv(file.choose()) # Creates a window for you to select the .csv file
tr_data <- tr_data %>% 
           as_tsibble(key = symbol, index = 'trading_day') %>% 
           fill(close, .direction="downup") # Interpolate missing values
```

### **1.1 Create a time-plot of your data and briefly describe it (max 75 words)** (1 points)

```{r}
# YOUR CODE GOES HERE
# DO NOT CREATE ADDITIONAL CODE SNIPPETS
```

------

YOUR ANSWER GOES HERE. MAX 50 WORDS.

------

### **2. Create two training datasets containing 80% and 60% of your data** (0 points)

For this, complete the code below. In the end you will have a single tsibble where each training dataset is identified by `.id`.

NOTE: we do not use stretch-tsibble in this case because we only want to produce two trading datasets.

```{r}
n_obs_80 <- # Your code goes here. Compute the integer number covering 80% of the observations
n_obs_60 <- # Your code goes here. Compute the integer number covering 60% of the observations
n_obs_list <- c(n_obs_60, n_obs_80)
train_sets <- tibble() # EMPTY TIBBLE

# For loop produces the training datasets and joins them together in a single dataframe
for (i in seq_along(n_obs_list)) {
  train_i <- tr_data %>% slice(1:n_obs_list[i]) %>% mutate(.id=i)
  train_sets <- bind_rows(train_sets, train_i)
}

# Turn into a tsibble for compatibility with the fable library.
train_sets <- as_tsibble(train_sets, key=.id, index=trading_day)
```

```{r}
n_obs_80 <- as.integer(nrow(tr_data) * 0.8)
n_obs_60 <- as.integer(nrow(tr_data) * 0.6)
n_obs_list <- c(n_obs_60, n_obs_80)
train_sets <- tibble() # EMPTY TIBBLE

# For loop produces the training datasets and joins them together in a single dataframe
for (i in seq_along(n_obs_list)) {
  train_i <- tr_data %>% slice(1:n_obs_list[i]) %>% mutate(.id=i)
  train_sets <- bind_rows(train_sets, train_i)
}

# Turn into a tsibble for compatibility with the fable library.
train_sets <- as_tsibble(train_sets, key=.id, index=trading_day)
```


### **3. Fit the following models to each of the training datasets:** (1 points)

* a Naïve model (`naive`).
* a Simple Exponential Smoothing model. Call it `ses_tr`
* a Trended Exponential Smoothing model with and additive dampled trend and no seasonality. Call it `holts_damped`
* an automatically selected ARIMA model. You still do not know about ARIMA models, so the formula for that is given below.

Name the object where you store the fitted objects `fit_train_sets`

```{r}
# YOUR CODE GOES HERE, DO NOT CREATE ADDITIONAL CODE SNIPPETS
fit_train_sets <-
  train_sets %>% 
  model(
    naive = # Your code goes here
    ses = # Your code goes here
    holts_damped =  # Your code goes here
    autoarima = ARIMA(close)
  )

fit_train_sets
```

```{r}
# YOUR CODE GOES HERE, DO NOT CREATE ADDITIONAL CODE SNIPPETS
fit_train_sets <-
  train_sets %>% 
  model(
    naive = NAIVE(close),
    ses = ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped = ETS(close ~ error("A") + trend("Ad") + season("N")),
    autoarima = ARIMA(close)
  )

fit_train_sets
```

### **4. Generate forecasts of 10 trading days for each of the trained models** (1 point)

The result of this step should be a `fable` or forecast table. Name it `fc_train_sets`

Add a column `h` to `fc_train_sets` indicating the forecast horizon of each forecast.

* After adding that column, apply the following command: `as_fable(response = "close", distribution = close)`. This is to ensure that the output after performing these operations is still a `fable`, which can be used in combination with the function `accuracy()`.
* Do not forget the command `ungroup()` as well.

```{r}
fc_train_sets <- 
  fit_train_sets %>% 
  # YOUR CODE GOES HERE
```

```{r}
fc_train_sets <- 
  fit_train_sets %>%
  forecast(h=5) %>%
  group_by(.id, .model) %>%
  mutate(h=row_number()) %>%
  as_fable(response = "close", distribution = close) %>% 
  ungroup()
```

### **5. Compute errors and error metrics for each training dataset

#### 5.1 Compute the absolute error and the absolute percent error (without averaging) at every point.

To do this, follow the steps below:


```{r}
# 0. Select only the Symbol, trading_day and close. Then rename the column "close"
# to close_data
aux1 <- 
  tr_data %>% 
  select(symbol, trading_day, close) %>% 
  rename(close_data = close)

# 1. Use a left_join() between fc_train_sets (left table) and tr_data (right table)
# to incorporate the actual value of the variable "close" to fc_train_sets. 
# Store the result in a new variable called "error_metrics".
# NOTE: since fc_train_sets already has a column called close, the result of the
# left_join() will have two columns: close.x and close.y to resolve that conflict.
# close.y should contain the values of the variable if you did things as described above
df_errors <- 
  

# 2. Create two new columns: abs_error and abs_perc_error corresponding to the
# absolute error and the absolute percentage error for each point forecast. 
df_errors <- ...
  
# 3. Check the result. It should have 80 rows and containt the abs_error
#    and the absolute percentage error associated to each forecast.
#   (4 models * 2 training datasets * 10 forecasts per model)
df_errors 

```

```{r}
# 0. Select only the Symbol, trading_day and close. Then rename the column "close"
# to close_data
aux1 <- 
  tr_data %>% 
  select(symbol, trading_day, close) %>% 
  rename(close_data = close)

# 1. Use a left_join() between fc_train_sets (left table) and aux1 (right table)
# to incorporate the actual value of the variable "close_data" to fc_train_sets. 
# Store the result in a new variable called "df_errors".
df_errors <- 
  fc_train_sets %>% 
  left_join(aux1, by="trading_day")

# 2. Create two new columns in df_errors: abs_error and abs_perc_error corresponding to the
# absolute error and the absolute percentage error for each point forecast. 
# COMPUTE THE ERROR FOR EACH POINT FORECAST, WITHOUT AVERAGING.
df_errors <- 
  df_errors %>% 
  mutate(
    abs_error = abs(close_data - .mean),
    abs_perc_error = abs((close_data - .mean) / close_data) * 100
  )
  
# 3. Check the result. It should have 80 rows and containt the abs_error
#    and the absolute percentage error associated to each forecast.
#   (4 models * 2 training datasets * 10 forecasts per model)
df_errors
```

#### 5.2 Produce the following graphs and answer the questions posed (2.5 points)

Yo are tasked with generating the following graphs: 

TWO GRAPHS FOR THE ABSOLUTE ERRORS.

The graphs shall be generated using the library `ggplot`.

These graphs shall fulfil the following:

- x-axis corresponds to the forecast horizon and its grid shows only integer values
- y-axis corresponds to the values of the absolute error or the percent error. **The y-axis on both graphs shall have the same scale to be able to compare results**.
- In summary, each graph shall have 4 lines, one line for each of the four models fitted depicting the absolute errors.

I recommend using `facet_wrap(".id")` to produce the two desired graphs, but you are free to proceed otherwise if you reach the same result.

```{r}
df_errors %>% 
  ggplot(aes(x = h, y = abs_error, colour = .model)) + 
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) + 
  scale_x_continuous(
    breaks=1:max(df_errors$h),
    minor_breaks=1:max(df_errors$h)
                     ) + 
  # Expand to compare the effect of the training dataset
  facet_wrap(".id") + 
   ggtitle("absolute errors")
```

TWO GRAPHS FOR THE ABSOLUTE PERCENTAGE ERRORS.

These graphs shall fulfil the following:

- x-axis corresponds to the forecast horizon and its grid shows only integer values
- y-axis corresponds to the values of the absolute percent error. **The y-axis on both graphs shall have the same scale to be able to compare results**.
- In summary, each graph shall have 4 lines, one line for each of the four models fitted depicting the absolute errors.

I recommend using `facet_wrap(".id")` as the last ggplot command to produce one graph for each trading dataset, but you are free to proceed otherwise if you reach the same result.

```{r}
df_errors %>% 
  ggplot(aes(x = h, y = abs_perc_error, colour = .model)) + 
  geom_point(alpha = 0.5) + 
  geom_line(alpha = 0.5) + 
  scale_x_continuous(
    breaks=1:max(df_errors$h),
    minor_breaks=1:max(df_errors$h)
                     ) + 
  # Expand to compare the effect of the training dataset
  facet_wrap(".id") + 
   ggtitle("absolute percent errors errors")
```

QUESTIONS ABOUT GRAPHS:

1. Does the training dataset have an effect on the errors?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

2. Does one of the models outperform the other? Are these results the same for both training datasets?

------

**YOUR ANSWER GOES HERE**
MAX 30 WORDS

------

#### 5.3 For each training dataset, generate the RMSE, MAE and MAPE of each model averaged over all forecast horizons.

To do this, use two alternatives:

**ALTERNATIVE 1: use `àccuracy(tr_data, by=...)` on fc_train_sets**. Store the result in `error_metrics_accuracy`

```{r}
error_metrics_accuracy <- 
  fc_train_sets %>% 
  accuracy(tr_data, by=c(".id", ".model")) %>% 
  arrange(.id, RMSE) %>% 
  select(.id, .model, .type, RMSE, MAE, MAPE)

error_metrics_accuracy
```

**ALTERNATIVE 2: using the abs_error and abs_perc_error columns in `df_errors`, group_by appropriately and use summarize + the formulas for the error metrics**. Store 

```{r}
error_metrics_manual <- 
  df_errors %>% 
  as_tibble() %>% 
  group_by(.id, .model) %>% 
  summarize(
    MAE = mean(abs_error, na.rm=TRUE),
    RMSE = sqrt(mean(abs_error^2, na.rm=TRUE)),
    MAPE = mean(abs_perc_error, na.rm=TRUE)
  ) %>% 
  ungroup() %>% 
  arrange(.id, RMSE)

error_metrics_manual
```

```{r}
# COMPARE METRICS
all.equal(error_metrics_manual$MAE, error_metrics_accuracy$MAE)
all.equal(error_metrics_manual$RMSE, error_metrics_accuracy$RMSE)
all.equal(error_metrics_manual$MAPE, error_metrics_accuracy$MAPE)
```

### **6. Instead of running the process for two training datasets separately, do cross validation** (2.5 points)

#### 6.1 Take all the steps necessary to obtain the cross validated MAE, RMSE and MAPE for each combination of model and forecast horizon. 

- The final table shall therefore have 4 x 10 rows (4 models and 10 forecasts per model). For each row, a column for MAE, another for RMSE and another for MAPE.

- The smallest training dataset shall contain 60% of the trading days.
- The trading datasets shall increase in steps of two trading days.

```{r}
# Create training datasets
init_obs <- as.integer(nrow(tr_data)*0.6)

tr_data_cv <- 
  tr_data %>% 
  stretch_tsibble(
    .init = init_obs,
    .step = 5
  )

# Fit models to all training datasets
fit_cv <- 
tr_data_cv %>% 
  model(
    naive = NAIVE(close),
    ses =  ETS(close ~ error("A") + trend("N") + season("N")),
    holts_damped = ETS(close ~ error("A") + trend("Ad") + season("N")),
    autoarima = ARIMA(close)
  ) 

# Produce forecasts for each training dataset
# and generate column h to segregate the accuracy metrics.
fc_cv <- 
  fit_cv %>% 
  forecast(h=5) %>% 
  group_by(.id, .model) %>% 
  mutate(h=row_number()) %>% 
  ungroup() %>% 
  as_fable(response="close", distribution=close) 

# Accuracy metrics for each model by forecast horizon:
summary_cv <- 
  fc_cv %>% 
    accuracy(tr_data, by = c("h", ".model")) %>% 
    select(h, .model, MAE, MAPE)
```

#### 6.2 Obtain the cross-validated accuracy metrics manually (layout)

### **7. Produce the following graphs and answer the questions** (2.5 points) 

Produce two graphs. One for the MAE and one for the MAPE. The graphs shall fulfill:

1. The x-axis is discrete and corresponds to the forecast horizon
2. The y-axis correspondds to the values of the error (MAE or MAPE depending on the graph)
3. One line per model, so in total we should have three lines in each graph.

- x-axis corresponds to the forecast horizon and its grid shows only integer values
- y-axis corresponds to the values of the MAE or the MAPE, depending on the graph.
- Each graph shall have 4 lines, one line for each of the four models fitted depicting the absolute errors.

```{r}
# Convert to long format to use with ggplot
summary_cv_long <- 
  summary_cv %>% 
    pivot_longer(
      cols = c("MAE", "MAPE"),
      names_to = "metric", 
      values_to = "value"
    ) 

# Figure for MAE abd MAPE
summary_cv_long %>% 
  # filter(metric %in% c("MAE", "MAPE")) %>%
  ggplot(aes(x = h, y = value, 
             colour = .model
             )
         ) + 
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) + 
  scale_x_continuous(
    breaks=1:max(summary_cv$h),
    minor_breaks=1:max(summary_cv$h)
                     ) +
  facet_wrap("metric", scales = "free_y") +
  ggtitle("Cross-validated MAE and MAPE") 
```

* What is the difference between these graphs and the ones we generated in question 5.2? Answer briefly below.

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

* Does one of the models outperform the other?

------

YOUR ANSWER GOES HERE: MAX 30 WORDS

------

### **8. Reflect on the results and present some conclusions** (0 point)

------

YOUR ANSWER GOES HERE.
MAX 100 WORDS.

------
