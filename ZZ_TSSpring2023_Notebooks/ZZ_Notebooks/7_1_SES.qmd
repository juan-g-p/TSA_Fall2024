---
title: "7_1_Simple_Exponential_Smoothing - Theory"
toc: true
toc-location: left
toc-depth: 6
self-contained: true
format: html
editor: source
params:
  print_sol: true
---

# References

1.  Hyndman, R.J., & Athanasopoulos, G., 2021. *Forecasting: principles and practice*. 3rd edition.
2.  Fable package documentation

-   <https://fable.tidyverts.org/index.html>
-   <https://fable.tidyverts.org/articles/fable.html>

# Note

This notebook is not original material, but rather based on ref. 1 with notes expanding on some of the concepts contained in the book.

# Libraries

```{r, warning= FALSE, message=FALSE}
library(fpp3)
library(patchwork) # to easily place graphs next to each other
```

# Exponential Smoothing - Intro

**Exponential smoothing forecasts, main idea**

- **Mean method**: every observation gets the same weight assigned
- **Naive method**: all weight assigned to the last observation
- **Exponential Smoothing:** Weighted averages of past observations with weights decaying exponentially as the observations get older.
    -   The more recent the observation, the higher its associated weight.

# Simple Exponential Smoothing (SES)

**USE:** data with **no clear trend or seasonal pattern**.

Example:

```{r}
algeria_economy <- global_economy %>%
  filter(Country == "Algeria")

algeria_economy %>%
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Exports: Algeria")
```

Data above does not have clear trend or seasonality. Among the benchmark methods, **naïve or mean methods** would be suitable for this data.

## Naïve vs. Average vs. Simple Exponential Smoothing

### Naïve

All future forecasts are equal to the last observed value. **The most recent observation is the only important one**.

$$
\hat{y}_{T+h|T} = y_{T}
$$

-   for $h = 1, 2, \dots$

### Mean method

All future forecasts are equal to a simple average of the observed data. **All observations are of equal importance, assigning them equal weights when generating the forecasts**

$$
\hat{y}_{T+h|T} = \frac1T \sum_{t=1}^T y_t
$$

-   for $h = 1, 2, \dots$

## Simple Exponential Smoothing

Between the naive and the average approach.

-   **Forecasts = weighted averages**.

    -   More recent observations have larger weights associated.
    -   Weights decrease exponentially the more distant in the past the observations.

$$
    \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots = \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} \tag{1}
$$
As you can see, the weights of simple exponential smoothing follow a geometric progression of ration $1 - \alpha$.

### Flat forecasts of simple exponential smoothing

Simple Exponential Smoothing has a "flat" forecast function. **All forecasts beyond the training data take the same value.**

$$
\hat{y}_{T+h|T} = \hat{y}_{T+1|T} =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} \tag{1} \qquad h=2,3,\dots.
$$

-   These forecasts will only be suitable if the time series has no trend or seasonal component

The **fitted values** on the other hand **will not be constant**. They will be *one-step ahead forecasts* based on all the preceding datapoints.

Further down below in the notebook we go deeper into these topics.

### Coefficients of the weighted average

-   Controlled by the parameter $\alpha$. This satisfies $0 \le \alpha \le 1$

-   **One-step ahead forecast for time** $T+1$: weighted average **of all of the observations** $y_1,\dots,y_T$

-   $\alpha$ is the parameter controlling the **rate at which weights decrease**.

    -   **If** $\alpha$ is small (close to 0), more weight is given to observations from the more distant past.
    -   **If** $\alpha$ is large (close to 1), more weight is given to more recent observations.
    -   **If** $\alpha = 1$, then $\hat{y}_{T+1|T}=y_T$ and the model becomes the naïve method. All the weight is assigned to the last observation.
    
To understand this, consider the previously explained smoothing process:

$$
    \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots = \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} \tag{1}
$$

-   The table below shows the weights attached to 5 observations for four different values of \alpha when forecasting using simple exponential smoothing.

|           | $\alpha=0.2$ | $\alpha=0.4$ | $\alpha=0.6$ | $\alpha=0.8$ |
|:----------|:-------------|:-------------|:-------------|:-------------|
| $y_{T}$   | 0.2000       | 0.4000       | 0.6000       | 0.8000       |
| $y_{T-1}$ | 0.1600       | 0.2400       | 0.2400       | 0.1600       |
| $y_{T-2}$ | 0.1280       | 0.1440       | 0.0960       | 0.0320       |
| $y_{T-3}$ | 0.1024       | 0.0864       | 0.0384       | 0.0064       |
| $y_{T-4}$ | 0.0819       | 0.0518       | 0.0154       | 0.0013       |
| $y_{T-5}$ | 0.0655       | 0.0311       | 0.0061       | 0.0003       |

-   The coefficients extend until the first term of the series.

-   **As the table above shows, if the series is sufficiently long, the sum of the coefficients of the simple exponential smoothing method is close to one**. In the limit, for $T\rightarrow \infty$ the sum of the coefficients is one. Lets examine this:

$$
\sum_{i=0}^{T-1}\alpha(1-\alpha)^j
$$

This corresponds to a geometric progression of ratio $1-\alpha \leq 1$. Therefore it has a finite sum hat we can compute as follows:

$S_n = \alpha+\alpha(1-\alpha)+\alpha(1-\alpha)^2+\dots+\alpha(1-\alpha)^{T-1} \tag{2}$ 

$(1-\alpha) S_n = \alpha(1-\alpha)+\alpha(1-\alpha)^2+\dots+\alpha(1-\alpha)^{T-1}+\alpha(1-\alpha)^{T} \tag{3}$

Substracting (2) - (3) we obtain: 

$$
\alpha-\alpha(1-\alpha)^{T} = S_n(1-(1-\alpha))=S_n\alpha \;\;\;\;\;\;\;\;(2)-(3)
$$

Hence, since $(1-\alpha)$ is smaller than 1, then $(1-\alpha)^T\xrightarrow[T \to \infty]{}0$ and we finally obtain:

$$
Sn = 1-(1-\alpha)^T \xrightarrow[T \to \infty]{} 1
$$

In the next sections, the equations of simple exponential smoothing are presented in two forms. It is shown that both are equivalent to the forecast equation (1),

### Weighted average form and fitted values

From eq (1) we have:

$$
    \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots = \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} \tag{1}
$$

Substituting $T$ by $T-1$ we obtain the following expression:

$$
    \hat{y}_{T|T-1} = \alpha y_{T-1} + \alpha(1-\alpha) y_{T-2} + \alpha(1-\alpha)^2 y_{T-3}+ \cdots = \sum_{j=0}^{T-2} \alpha(1-\alpha)^j y_{T-1-j} \tag{1*}
$$

Comparing $(1)$ and $(1*)$ we see that the forecast at time $T+1$ is in fact a weighted average between the most recent observation $y_T$ and the previous forecast. That is, if we multiply equation $(1*)$ by $(1-\alpha)$ and add to it $\alpha y_t$, we recover equation $(1)$

$$
\hat{y}_{T+1|T} = \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}\\ \tag{1**}
$$

-   $\hat{y}_{T+1|T}$ forecast at time T+1
-   $y_T$ is the most recent observation
-   $\hat{y}_{T|T-1}$ is the previous forecast
-   $\alpha$ is the smoothing parameter.

Equation $(1**)$ is simply another way to write $(1)$, but it provides us with a simple equation for one-step ahead forecasts. Since **fitted values** are one-step forecasts on the training data to which the model is fitted, we may use $(1**)$ to express the fitted values as follows:

$$
\hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1}
$$

-   for $t = 1, \dots, T$ (for the values of $t$ that encompass the training data).

We can write this equation for each of the fitted values. However, **the first fitted value at time t=0**, denoted by $\hat{y}_{1|0} = l_0$, needs to be estimated or the process cannot start, since the equation relies on the fitted value at time $t$ ($\hat{y}_{t|t-1}$) to compute the fitted value at $t+1$ ($\hat{y}_{t+1|t}$). 

Once we estimate $\hat{y}_{1|0} = l_0$, we may write the following equation for each of the fitted values usng equation $(1**)$

```{=tex}
\begin{align*}
  \hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) \ell_0\\
  \hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1}\\
  \hat{y}_{4|3} &= \alpha y_3 + (1-\alpha) \hat{y}_{3|2}\\
  \vdots\\
  \hat{y}_{T|T-1} &= \alpha y_{T-1} + (1-\alpha) \hat{y}_{T-1|T-2}\\
  \hat{y}_{T+1|T} &= \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}.
\end{align*}
```

Now, substituting each equation into the following equation, we reach:

```{=tex}
\begin{align*}
  \hat{y}_{3|2}   & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right]              \\
                 & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0                          \\
  \hat{y}_{4|3}   & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0]\\
                 & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0 \\
                 & ~~\vdots                                                                           \\
  \hat{y}_{T+1|T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0} \tag{4}.
\end{align*}
```

Since $1-\alpha \leq 1$, the last term becomes tiny for large enough $T$ and equation (4) converges equation (1).

We have thus obtained a simple algorithm to compute the fitted values based on an initial estimation $\hat{y}_{1|0} = l_0$.

### Component form

This form is more useful from a computational standpoint. For Simple Exponential Smoothing it might seem unnecessary, but it will be most convenient when more components are added in the trended and seasonal forms of exponential smoothing.

For simple exponential smoothing, the component form consists of:

1.  A forecast equation
2.  A smoothing equation for each of the components included in the method.

The forecast equation can be deduced from the fact that, for simple exponential smoothing, the forecasts will be flat. Considering this, the forecast equation will simply be:

```{=tex}
\begin{align*}
\text{Forecast equation}  && \hat{y}_{t+h|t} & = \ell_{t}
\end{align*}
```

where: 

* $l_t$ is the level of the time series at time $t$. Think of it as the smoothed value of the time series at time t obtained during the fitting of the SES model. Continue reading if this does not makes much sense at the current point.

To obtain the smoothing equation, we start with the equation we used for the fitted values:

$$
\hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1}
$$

We may now substitute: 

* $\hat{y}_{t|t-1}$ by $l_{t-1}$ following the forecast equation
* $\hat{y}_{t+1|t}$ by $l_{t}$ 

Doing this yields the smoothing equation for the **level of the time series**

```{=tex}
\begin{align*}
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
```

-   $l_t$ is the level (or smoothed value) of the series at time $t$.

Together, the *forecast equation* and the *smoothing equation* constitute the **component form of simple exponential smoothing**.

```{=tex}
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+h|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
```

**IMPORTANT:**

* setting $h=1$ gives the fitted values.
* setting $t=T$ gives the true forecasts beyond the trainig data.

For simple exponential smoothing the equations can be interpreted as follows:

-   **The forecast equation** shows that the forecast value at time $t+1$ is the estimated level at time $t$
-   **The smoothing equation for the level** (a.k.a level equation) gives the estimated level of the series at time t as a **weighted average between the value of the series at time $t$ (that is, $y_t$) and the level of the time series at $t-1$ (that is, $l_{t-1}$)**

### Flat forecasts of simple exponential smoothing

Simple Exponential Smoothing has a "flat" forecast function:

$$
\hat{y}_{T+h|T} = \hat{y}_{T+1|T}=\ell_T, \qquad h=2,3,\dots.
$$

**All forecasts beyond the training data take the same value, equal to the last level component, the smoothed value.**

-   Since the forecasts are constant, SES will only be suitable if the time series has no trend or seasonal component.

An example at the end of the notebook will clarify this further

### Finding best values for $l_0$ and $\alpha$

To apply any exponential smoothing method, some values need to be chosen. Specifically:

-   The smoothing parameters
-   The initial values.

For simple exponential smoothing, these values are: 

-   $\alpha$ (the smoothing paramenter) 
-   $l_0$ (the initial level or first fitted value)

For more complex exponential smoothing methods there will be more than one smoothing parameter and more than one initial component.

**Choice of parameters**

-   Sometimes previous experience guides the choice of these parameters.
-   Usually the observed data is used to estimate them.
    -   The **sum of squared residuals** is minimized to choose these parameters. 
        -   NOTE: the **sum of squared residuals** is usually known as the **sum of squared errors (SSE)**, although technically residuals $\neq$ errors.

$$
  \text{Residuals} = e_t=y_t - \hat{y}_{t|t-1}
$$
$$
\text{SSE} = \sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2=\sum_{t=1}^Te_t^2.
$$

For Simple Exponential Smoothing this is a non-linear minimization problem and requires the use of an external algorithm to perform the minimization of SSE. The process could be summarized as follows:

```{r, echo=FALSE, out.width='70%', fig.align="center", fig.cap=""}
knitr::include_graphics('./figs/ses_fit.png')
```

You may recall that for Linear Regression there are explicit formulas providing the solution the problem of minimizing the SSE. This is not the case in SES.

### Model specification in `fable()` - example

#### 1. Fitting the model

Exponential Smoothing Methods can be fitted using the `ETS()` function of the `fable` library. `ETS` is an acronym for two things:

1. *E*xponen*T*ial *S*moothing
2. *Error* - *Trend* - *Season* - this will be useful to write the model formula, as we will see in the examples. It is related to the more elaborate forms of exponential smoothing that include `trend` and `seasonality`.

Two functions are used to fit an ETS() model:

-   **model()** (also from `fable`) trains the specified model definition to a given dataset.
-   **`ETS()`** returns an ETS model specified by the formula specified via its arguments

The details of these functions can be queried using `?ETS()` or `?model()` in the console, but it is best to examine their use through an example. We will resort to the example of the Algerian economy we were dealing with before.

```{r}
# Estimate parameters
fit <- algeria_economy %>%
  model(ETS(Exports ~ error("A") + trend("N") + season("N")))
```

#### 2. Computing the fitted values

Once the model has been fitted, we can use `augment()` as usual to examine the fitted values and the residuals:

```{r}
fitted_vals <- 
  fit %>% augment() %>% select(Year, Exports, .fitted)

fitted_vals
```

#### 3. Interpreting the parameters

Remember that, when fitting the model, a value for $\alpha$ and $l_{t0}$ (initial level) has been internally chosen by minimizing the sum of square errors. These are the model parameters for simple exponential smoothing and can be queried using the function `tidy()` on the model:

```{r}
# Print out the model parameters
ses_params <- tidy(fit)
ses_params

# Store alpha
alpha <- ses_params$estimate[1]
```

In this case we have $\alpha = 0.84$ and $l_{t0} = 39.54$.

We can use the parameter $\alpha$ to analyze the distribution of the weights for the different observations. Remember that the weighted average form eq $(1)$:

$$
    \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots = \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} \tag{1}
$$

The weight associated to the observation $y_{T-j}$ is therefore:

$$
\sum_{j=0}^{T-1} \alpha(1-\alpha)^j
$$
With this formula we may use a for loop through the length of the fitted values to compute the weights associated to each observation:

```{r}
weights <- vector("double") 
for (i in seq(1:nrow(fitted_vals))){
  # Compute the weight associated to each observation
  weights[[i]] <- alpha*((1-alpha)**(i-1))
}

# Add this column to the fitted values
fitted_vals$weight = rev(weights)

# Reverse de order of the dataframe to depict it more clearly
fitted_vals <- 
  fitted_vals %>% arrange(desc(Year))

fitted_vals
```

We can see that the most recent observation gets assigned a weight of $\alpha = 0.8399$. The observation in 2016 a weight of $\alpha(1-\alpha)=0.1344$, the observation in 2015 gets assigned a weight of $\alpha(1-\alpha)^2 = 0.0215$...

In fact, these first three observations together get assigned a weight of:

```{r}
sum(fitted_vals$weight[1:3]) 
```
The remaining difference up to 1 is distributed among the rest of the observations. That is, as expected:

* More recent observations get assigned more weight.
* Observations more distant in the past get assigned weights that reduce exponentially.

As a final check because the length of fitted values is substantial (58), we expect the sum of the weights to be very close to 1. We had shown this with equation $(4)$:

```{r}
sum(fitted_vals$weight)
```

#### 4. Reconstructing the fitted values using $\alpha$ and $l_0$

Using the component form of the equation and the estimates of $\alpha$ and $l_0$ returned by the model, we can compute the fitted values. This corresponds to step 2 of the last iteration of the minimisation of SSE process used to find appropriate values for $\alpha$ and $l_0$. Let us reproduce the diagram below again:

```{r, echo=FALSE, out.width='70%', fig.align="center", fig.cap=""}
knitr::include_graphics('./figs/ses_fit.png')
```

Considering that

-   Fitted values can be understood has 1-step forecasts on the training data $y_{t|t-1}$
-   The initial level $l_0$ has been chosen by the function minimizing the sum of square errors.

We may substitute $h=1$ in the component form of the equations and use the formulas to compute the fitted values in the table that follows:

```{=tex}
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+1|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
```

| Year | Time | Observation | Level | Forecast          |
|:-----|:-----|:------------|:------|:------------------|
|      | $t$  | $y_t$       | $l_t$ | $\hat{y}_{t|t-1}$ |
| 1959 | 0    |             | 39.54 |                   |
| 1960 | 1    | 39.04       | 39.12 | 39.54             |
| 1961 | 2    | 46.24       | 45.10 | 39.12             |
| 1962 | 3    | 19.79       | 23.84 | 45.10             |
| 1963 | 4    | 24.68       | 24.55 | 23.84             |
| 1964 | 5    | 25.08       | 25.00 | 24.55             |
| 1965 | 6    | 22.60       | 22.99 | 25.00             |
| 1966 | 7    | 25.99       | 25.51 | 22.99             |
| 1967 | 8    | 23.43       | 23.77 | 25.51             |
|      | ⋮    | ⋮           | ⋮     | ⋮                 |
| 2014 | 55   | 30.22       | 30.80 | 33.85             |
| 2015 | 56   | 23.17       | 24.39 | 30.80             |
| 2016 | 57   | 20.86       | 21.43 | 24.39             |
| 2017 | 58   | 22.64       | 22.44 | 21.43             |

#### Exercise: use the excel provided in class to compute these tables:

#### 5. Computing forecasts

Once the model has been fit, we may produce forecasts with the usual syntax:

```{r}
# Generate 5 step forecasts
fc <- fit %>%
  forecast(h = 5)
```

```{r}
fc %>%
  autoplot(algeria_economy) +
  geom_line(aes(y = .fitted), col="#D55E00",
            data = augment(fit)) +
  labs(y="% of GDP", title="Exports: Algeria") +
  guides(colour = "none")
```

We may build a similar table for our forecasts:

| Year | Time | Observation | Level | Forecast                |
|:-----|:-----|:------------|:------|:------------------------|
|      | $h$  |             |       | $\hat{y}_{T+h\vert{T}}$ |
| 2018 | 1    |             |       | 22.44                   |
| 2019 | 2    |             |       | 22.44                   |
| 2020 | 3    |             |       | 22.44                   |
| 2021 | 4    |             |       | 22.44                   |
| 2022 | 5    |             |       | 22.44                   |

This should further clarify the point that, beyond the training data, forecasts become "flat" for the simple exponential smoothing algorithm. This is the design of the algorithm.

# Summary

-   Exponential smoothing generates forecasts as weighted averages.
    -   More recent observations have larger weights associated to them
    -   Weights decrease exponentially the more distant in the past the observations.
    
-   Simple Exponential Smoothing
    -   May be used for **data with no clear trend or seasonal pattern**
    -   In-between the **naïve** and the **average** method.
    -   Fitting the model requires minimizing the SSE to determine the values of $\alpha$ and $l_{t0}$ (model parameters)
    -   This optimization is performed automatically by the `fable()` library, with the possibility of tweaking the optimization criteria (this will be seen in the next semester).
    -   Simple exponential smoothing produces forecasts that are flat in nature and equal to the last level component. Hence it is only suitable if the time series has no trend or seasonal component.