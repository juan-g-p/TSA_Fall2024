---
title: "Untitled"
format: html
editor: source
---

```{r}
library(fpp3)
```

## 1. Transform data to an appropriate granularity (1.5 points)

The `pedestrian` dataset is loaded along with the fpp3 library

This dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.

-   **Sensor**: name of the sensor (key of the tsibble).
-   **Date_time**: time when the pedestrian couns are recorded (time index).
-   **Date**: date at which the pedestrian counts are recorded.
-   **Time**: hour associated with **Date_Time**.
-   **Counts**: hourly pedestrian counts.

We want to aggregate this data to compute the **total number of pedestrians measured by each sensor in a day**. More specifically, we want to compute this for:

1.  Only two sensors (filter out the rest of the sensors):

    -   "Southern Cross Station"
    -   "Bourke Street Mall (North)"

2.  The time period comprised between "2016-07-01" and "2016-10-25".

**WRITE BELOW THE CODE THAT PERFORMS THIS AGGREGATION**.

-   Note that the variable `Date` already gives you the day to which each data point corresponds. Use it.

```{r}
## YOUR CODE GOES HERE

## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

**CREATE A SINGLE GRAPH CONTAINING ONE TIME PLOT FOR EACH OF THE TWO SENSORS (single graph with two curves)**

-   Use minor breaks every week and labelled breaks every 5 weeks

```{r}
## YOUR CODE GOES HERE
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

**FOR EACH SENSOR, DETERMINE THE DAY AT WHICH THE GREATEST NUMBER OF PERSONS IS MEASURED**

```{r}
## YOUR CODE GOES HERE
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

## 2. Load the data (0 points)

The snippet below loads the daily aggregated data measuring the number of pedestrians at the `Southern Cross Station` sensor in the city of Melbourne. It also transforms it to a tsibble.

Run the code and a pop-up window will appear. Select the file **scs_daily_pedestrian.csv** that has been provided along with the exam.

```{r, eval=FALSE}
scs_pedestrians <- 
  read.csv(file.choose()) %>% 
  mutate(Date = as.Date(Date)) %>% 
  as_tsibble(index=Date)

scs_pedestrians
```

## 3. Time plot (2 points)

Create a time plot of the series, set minor breaks every week and major breaks every 5 weeks.

```{r}
## YOUR CODE GOES HERE
scs_pedestrians%>% autoplot()+
  scale_x_yearmonth(breaks = "5 weeks", minor_breaks = "1 week"
)+  theme(axis.text.x = element_text(angle = 90))
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

Based on the time-plot, which **length of the seasonal period would you consider from this data and why?. Specify the value you would use for m, what we have called the length of the seasonal period**.

------------------------------------------------------------------------

The Seasonal period has a length of 1 week. This is due to the fact that the visual pattern repeats itself every week, summing up to 5 patterns each major break.

------------------------------------------------------------------------

## 4. Time Series decomposition (3 points)

First, run the following code to create a new variable within `scs_pedestrians` containing the mean value of the time series:

```{r}
scs_pedestrians$mean_val = mean(scs_pedestrians$Count, na.rm = TRUE)
scs_pedestrians
```

### 4.1 Use an appropriate moving average to compute the trend of the time-series as **classical_decomposition** would. Then depict the time series along with this trend estimate and the `mean_val` computed just before. Use a dahsed line for the `mean_val`.

-   Store the trend in a column called `trend_class` within `scs_pedestrians`

```{r}
## YOUR CODE GOES HERE

  
unemp_new <- 
  scs_pedestrians %>%
  mutate(
    # Unbalanced window for the first 4-MA: 1 point to the left, two
    # points to the right
    `7-MA` = slider::slide_dbl(Count, mean,
                .before =3, .after = 3, .complete = TRUE)
  )
unemp_new
unemp_new %>%
  autoplot(Count, colour = "gray") + geom_line(aes(y = `7-MA`), colour = "#D55E00") + geom_line(aes(y = mean_val ), linetype="dashed") 
  




## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

### 4.2 Based on the previous grqph, which of the following models would you pick to produce forecasts of up to 1 week ahead?

-   Mean model
-   Drift
-   Seasonal Naive model

------------------------------------------------------------------------

As the trend is pretty constant and is not ither positive or negative, I would choose a model that will just replicate the seasonal component (very strong in the time series). Therefore, I will choose the Seasonal Naive. 

------------------------------------------------------------------------

## 5. Fitting models (3.5 points)

### 5.1 Fit the following models and store the result in a variable called `fit`.

Use the following names

-   `mean`: mean model
-   `drift`: drift model
-   `snaive`: seasonal naive model

**The three models must be included within the same fitted object**.

```{r}
## YOUR CODE GOES HERE
mean <- 
  scs_pedestrians %>% 
  model(
    mean = MEAN(Count)
    )
mean


drift <- scs_pedestrians %>% model(Drift = RW(Count ~ drift()))
drift

snaive<- scs_pedestrians %>% model(SNaive = SNAIVE(Count ~ lag("week")))
snaive

## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

#### 5.2 Perform forecasts of up to one week ahead. The produce two figures

1.  Forecasts of the seasonal na√Øve model (including prediction intervals)
2.  Forecasts of the mean model (including prediction intervals)

FEEDBACK: WHERE ARE THE PLOTS!!

```{r}
## YOUR CODE GOES HERE
fitted_vals_mean <- 
  mean %>% 
  augment()
forecasts <- mean %>% forecast(h = 7)
forecasts

fitted_vals_snaive <- snaive %>% augment()
forecasts <- snaive %>% forecast(h = 7)
forecasts 

## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

#### 5.3 Residuals of the mean model

Analyze the residuals of the mean model. Specifically do the following

##### 5.3.1. Produce an autocorelation plot (a.k.a correlogram) of:

1.  The original time series `Count` in `scs_pedestrians`.
2.  The innovation residuals of the `mean` model.

FEEDBACK: SEE SOLUTION, YOU ARE NOT DOING WHAT I REQUEST OR UNDERSTANDIG WHAT  I MEAN

```{r}
## YOUR CODE GOES HERE
scs_pedestrians %>% 
  ACF(Count) %>% 
  autoplot() 

scs_decomp <- scs_pedestrians %>%
  model(
    classical = classical_decomposition(Count, type = "additive")
  ) 
scs_comp <- 
  scs_decomp %>% 
  components()
scs_comp

# Examine the output

scs_comp %>%
  ACF(random) %>% 
  autoplot() 


## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

Then compare both ACFs below

------------------------------------------------------------------------

The first ACF plot (original) reveals a white noise like distribution of the data. In other words, the data is not very correlated. The remainder is not white noise like. There is a lot of autocorrelation, meaning a lot of structure/information has not been captured by neither the trend nor the seasonal component and has leaked to the remainder component. We want the remainder to be as random as possible.

------------------------------------------------------------------------

##### 5.3.2 Analyze whether the residuals of the mean model are biased or not

```{r}
## YOUR CODE GOES HERE
residuals <- scs_pedestrians %>% model(
                                Mean = MEAN(Count)
                               )
residuals
model_vals <- residuals %>% augment()
residuals %>% 
  select(Mean) %>% # Selects the Mean model
  gg_tsresiduals()
mean(model_vals$.resid)
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

------------------------------------------------------------------------

Looking at the distribution of the residuals we can say:

They are white noise like ( the residuals are not apparently correlated, and show quite a randomness)

The distribution of these residuals is homoscedastic as the variance is pretty similar among all the data frame

Now the mean of the residuals is pretty close to 0; advocating for an unbiased model.

feedback: just asked you to compute the actual mean of the residuals


------------------------------------------------------------------------

FEEDBACK: fairly good but you need to pay more attention to what I ask.

I would also keep on improving code and pay attention in class
