---
title: "MAXIMAs MIDTERM"
format: html
editor: source
---

```{r}
library(fpp3)
```

## 1. Transform data to an appropriate granularity (1.5 points)

The `pedestrian` dataset is loaded along with the fpp3 library

This dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.

* **Sensor**: name of the sensor (key of the tsibble).
* **Date_time**: time when the pedestrian couns are recorded (time index).
* **Date**: date at which the pedestrian counts are recorded.
* **Time**: hour associated with **Date_Time**.
* **Counts**: hourly pedestrian counts.

We want to aggregate this data to compute the **total number of pedestrians measured by each sensor in a day**. More specifically, we want to compute this for: 

1. Only two sensors (filter out the rest of the sensors):

    * "Southern Cross Station"
    * "Bourke Street Mall (North)"
    
2. The time period comprised between "2016-07-01" and "2016-10-25".

**WRITE BELOW THE CODE THAT PERFORMS THIS AGGREGATION**. 

  * Note that the variable `Date` already gives you the day to which each data point corresponds. Use it.

```{r}
## YOUR CODE GOES HERE

pedestrian 


filtered <- pedestrian %>%
  filter(Sensor =="Southern Cross Station" | Sensor == "Bourke Street Mall (North)",
         Date >= as.Date("2016-07-01"), Date <= as.Date("2016-10-25")) %>%
  group_by(Sensor, Date) %>%
  summarise(Daily_Total = sum(Count))







# filter(
 #   (year >= 1990) & (year <= 2010),
  #  (sex == "F")


## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

**CREATE A SINGLE GRAPH CONTAINING ONE TIME PLOT FOR EACH OF THE TWO SENSORS (single graph with two curves)**

  * Use minor breaks every week and labelled breaks every 5 weeks

```{r}
## YOUR CODE GOES HERE
ggplot(filtered, aes(x = Date, y = Daily_Total, color = Sensor)) +
  geom_line() +
  scale_x_date(date_breaks = "5 weeks", date_minor_breaks = "1 week") 

## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

**FOR EACH SENSOR, DETERMINE THE DAY AT WHICH THE GREATEST NUMBER OF PERSONS IS MEASURED**

```{r}
## YOUR CODE GOES HERE

# 3. Find the day with the highest count for each sensor
highest_day <- filtered %>%
  group_by(Sensor) %>%
  top_n(1, Daily_Total)


print("For Bourke:")
print(highest_day$Daily_Total[1])

print("For Southern:")
print(highest_day$Daily_Total[2])




## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

FEEDBACK: sorry... but in the end you failed to aggregate per day. You still have more than one datapoint per day. The concept of using group_by was present, but you need to practice it


## 2. Load the data (0 points)

The snippet below loads the daily aggregated data measuring the number of pedestrians at the `Southern Cross Station` sensor in the city of Melbourne. It also transforms it to a tsibble.

Run the code and a pop-up window will appear. Select the file **scs_daily_pedestrian.csv** that has been provided along with the exam.

```{r, eval=FALSE}
scs_pedestrians <- 
  read.csv(file.choose()) %>% 
  mutate(Date = as.Date(Date)) %>% 
  as_tsibble(index=Date)

scs_pedestrians
```

## 3. Time plot (2 points)

Create a time plot of the series, set minor breaks every week and major breaks every 5 weeks.

```{r}
## YOUR CODE GOES HERE

library(readr)
library(patchwork) # Used to manage the relative location of ggplots
library(GGally)

scs_pedestrians %>% autoplot() +
  scale_x_yearweek(date_breaks = "5 weeks" ,minor_breaks = "1 week") # check that yearweek is correct!

 
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

Based on the time-plot, which **length of the seasonal period would you consider from this data and why?. Specify the value you would use for m, what we have called the length of the seasonal period**.

------

I would use 7 for m, meaning one week as we can see that the pattern (the datatype data that) is reoccuring every week (can be seen in the minor breaks)
------

## 4. Time Series decomposition (3 points)

First, run the following code to create a new variable within `scs_pedestrians` containing the mean value of the time series:

```{r}
scs_pedestrians$mean_val = mean(scs_pedestrians$Count, na.rm = TRUE)

scs_pedestrians
```

### 4.1 Use an appropriate moving average to compute the trend of the time-series as **classical_decomposition** would. Then depict the time series along with this trend estimate and the `mean_val` computed just before. Use a dahsed line for the `mean_val`.

* Store the trend in a column called `trend_class` within `scs_pedestrians`

```{r}
## YOUR CODE GOES HERE

 decomp_model <- scs_pedestrians %>% 

    mutate(
    
  # 7-MA average bc odd seasonality
    trend_class = slider::slide_dbl(Count , mean,
                .before = 3, .after = 3, .complete = TRUE)
  ) 


decomp_model


decomp_model %>%
  autoplot(Count) +
  geom_line(aes(y = trend_class), colour = "#D55E00") + 
  geom_line(aes(y = mean_val), colour = "blue", linetype = "dashed" )



## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

### 4.2 Based on the previous grqph, which of the following models would you pick to produce forecasts of up to 1 week ahead?

* Mean model
* Drift
* Seasonal Naive model

------

I would use a seasonal naive model due to the fact that we have strong seasonality, which the other models wouldnt capture properly.We use seasonal naive to fit the seasonal component as well. 

------

## 5. Fitting models (3.5 points)

### 5.1 Fit the following models and store the result in a variable called `fit`.

Use the following names

* `mean`: mean model
* `drift`: drift model
* `snaive`: seasonal naive model

**The three models must be included within the same fitted object**.

```{r}
## YOUR CODE GOES HERE


fit <-
  scs_pedestrians %>% 
  model(
    snaive = SNAIVE(Count),
    mean = MEAN(Count),
    drift = RW(Count ~ drift())
  )
  
fit 
## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

#### 5.2 Perform forecasts of up to one week ahead. The produce two figures

1. Forecasts of the seasonal na√Øve model (including prediction intervals)
2. Forecasts of the mean model (including prediction intervals)

```{r}
## YOUR CODE GOES HERE

# fc_snaive <- fit  %>% forecast ( h = 7) %>% filter ( .model == "snaive")
#fc_mean <- fit %>% forecast ( h = 7 ) %>% filter ( .model == "mean") 


fc <- fit %>% forecast (h = 7) 

fc %>% 
filter(.model == "snaive") %>% 
  autoplot(scs_pedestrians) 

fc %>% 
  filter (.model == "mean")%>% 
  autoplot(scs_pedestrians)


## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

#### 5.3 Residuals of the mean model

Analyze the residuals of the mean model. Specifically do the following

##### 5.3.1. Produce an autocorelation plot (a.k.a correlogram) of:

1. The original time series `Count` in `scs_pedestrians`.
2. The innovation residuals of the `mean` model.

```{r}
## YOUR CODE GOES HERE

scs_pedestrians %>%
  ACF(Count) %>% autoplot()
 


 fit %>% select(mean) %>% augment() %>% ACF() %>% autoplot


## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

Then compare both ACFs below

------

They are the same. Both are not white noise.  
Why? Because the autocorrelation of the data stays the same even with the mean model. 
The residuals look exactly like the time plot, only shifted by the mean of the time series. FEEDBACK: PERFECT.
Hence we are barely using information in the model. To fix this we would need to change the model.
In the time plot the residuals are simply the time series minus its mean, so they have a trend and all the features of the original time series.

------

##### 5.3.2 Analyze whether the residuals of the mean model are biased or not

```{r}
## YOUR CODE GOES HERE
model_vals <- fit %>% augment() 
  
  model_vals %>% 
  as_tibble() %>%
  filter(.model == "mean") %>%
  summarise(mean_of_res = mean(.innov, na.rm = TRUE))
  
model_vals


model_vals <- fit %>% augment() %>% 
  filter(.model == "mean") 
  #summarise(mean = mean(.innov, na.rm = TRUE))

0.7
# Generate qq_plot
p1 <- ggplot(model_vals, aes(sample = .innov)) + 
        stat_qq() + 
        stat_qq_line()

# # Generate box-plot
p2 <- ggplot(data = model_vals, aes(y = .innov)) +
      geom_boxplot(fill="light blue", alpha = 0.7) +
      stat_summary(aes(x=0), fun="mean", colour= "red") # Include the mean

library (patchwork)
p1 +p2

## DO NOT CREATE ADDITIONAL CODE SNIPPETS, KEEP EVERYTHING IN A SINGLE SNIPPET.
```

------

The histogram, qq-plot and box-plot clearly show how the residuals are not normal. However, if we compute the mean manually we see that the mean is close to being zero (a bad model can be unbiased).

So it is unbiased, because the residuals are the series shifted by the mean of the series. We could do t test to see this further.

------

FEEDBACK: ONE OF THE BEST EXAMS. A PITY ABOUT THE FIRST POINT. SEE SOLUTION AND UNDERSTAND GROUP_BY AND INDEX_BY IN DETAIL.