---
title: "Untitled"
format: html
editor: source
---

```{r}
library(fpp3)
```

## 1. Transform data to an appropriate granularity (1.5 points)

The `pedestrian` dataset is loaded along with the fpp3 library

This dataset contains hourly pedestrian counts from 2015-01-01 to 2016-12-31 at 4 sensors in the city of Melbourne.

* **Sensor**: name of the sensor (key of the tsibble).
* **Date_time**: time when the pedestrian couns are recorded (time index).
* **Date**: date at which the pedestrian counts are recorded.
* **Time**: hour associated with **Date_Time**.
* **Counts**: hourly pedestrian counts.

We want to aggregate this data to compute the **total number of pedestrians measured by each sensor in a day**. More specifically, we want to compute this for: 

1. Only two sensors (filter out the rest of the sensors):

    * "Southern Cross Station"
    * "Bourke Street Mall (North)"
    
2. The time period comprised between "2016-07-01" and "2016-10-25".

**WRITE BELOW THE CODE THAT PERFORMS THIS AGGREGATION**. 

  * Note that the variable `Date` already gives you the day to which each data point corresponds. Use it.

```{r}
sensor_count = pedestrian %>%
  group_by(Sensor) %>%
  index_by(day = date(Date)) %>%
  filter(Sensor == "Southern Cross Station" | Sensor == "Bourke Street Mall (North)") %>%
  filter(Date >= date("2016-07-01") & Date <= date("2016-10-25")) %>%
  summarise(Total_Daily_Count = sum(Count))
  
```

**CREATE A SINGLE GRAPH CONTAINING ONE TIME PLOT FOR EACH OF THE TWO SENSORS (single graph with two curves)**

  * Use minor breaks every week and labelled breaks every 5 weeks

```{r}
sensor_count %>%
  autoplot() +
  scale_x_date(breaks = "5 weeks",
                   minor_breaks = "1 week") +
  labs(y = "Count of pedestrians",
       title = "Number of pedestrians over time")
```

**FOR EACH SENSOR, DETERMINE THE DAY AT WHICH THE GREATEST NUMBER OF PERSONS IS MEASURED**

```{r}
sensor_count %>%
  as_tibble() %>%
  group_by(Sensor) %>%
  filter(Total_Daily_Count == max(Total_Daily_Count)) %>%
  ungroup() %>%
  select(Sensor,day)
```

## 2. Load the data (0 points)

The snippet below loads the daily aggregated data measuring the number of pedestrians at the `Southern Cross Station` sensor in the city of Melbourne. It also transforms it to a tsibble.

Run the code and a pop-up window will appear. Select the file **scs_daily_pedestrian.csv** that has been provided along with the exam.

```{r, eval=FALSE}
scs_pedestrians <- 
  read.csv(file.choose()) %>% 
  mutate(Date = as.Date(Date)) %>% 
  as_tsibble(index=Date)

scs_pedestrians
```

## 3. Time plot (2 points)

Create a time plot of the series, set minor breaks every week and major breaks every 5 weeks.

```{r}
scs_pedestrians %>% 
  autoplot() +
  scale_x_date(breaks = "5 weeks",
                   minor_breaks = "1 week") +
  labs(y = "Count of pedestrians",
       title = "Number of pedestrians over time")
```

Based on the time-plot, which **length of the seasonal period would you consider from this data and why?. Specify the value you would use for m, what we have called the length of the seasonal period**.

------

We can see a pattern repeating every week. Since we have daily data, the length of the seasonal period would be 7 days (m=7)

------

## 4. Time Series decomposition (3 points)

First, run the following code to create a new variable within `scs_pedestrians` containing the mean value of the time series:

```{r}
scs_pedestrians$mean_val = mean(scs_pedestrians$Count, na.rm = TRUE)

scs_pedestrians
```

### 4.1 Use an appropriate moving average to compute the trend of the time-series as **classical_decomposition** would. Then depict the time series along with this trend estimate and the `mean_val` computed just before. Use a dahsed line for the `mean_val`.

* Store the trend in a column called `trend_class` within `scs_pedestrians`

```{r}
scs_pedestrians =
  scs_pedestrians %>%
  mutate(
    trend_class = slider::slide_dbl(Count, mean,
                .before = 3, .after = 3, .complete = TRUE)
  )

scs_pedestrians %>%
  autoplot(Count) +
  geom_line(aes(y = mean_val), colour = "blue", linetype = "dashed") +
  geom_line(aes(y = trend_class), colour = "red") 

```

### 4.2 Based on the previous grqph, which of the following models would you pick to produce forecasts of up to 1 week ahead?

* Mean model
* Drift
* Seasonal Naive model

------

Seasonal Naive model because since we have strong weekly seasonality it will be the only one including this seasonal pattern in the forecast while both mean and drift will produce flat forecatss (although drift will account for the trend). Moreover, since there is not a strong trend, it won't affect that the seasonal naive model doesn't account for the trend component in its forecasts. 

------

## 5. Fitting models (3.5 points)

### 5.1 Fit the following models and store the result in a variable called `fit`.

Use the following names

* `mean`: mean model
* `drift`: drift model
* `snaive`: seasonal naive model

**The three models must be included within the same fitted object**.

```{r}
fit = 
  scs_pedestrians %>%
  model(
    mean_m = MEAN(Count),
    drift_m =  RW(Count ~ drift()),
    snaive_m = SNAIVE(Count)
    )

```

#### 5.2 Perform forecasts of up to one week ahead. The produce two figures

1. Forecasts of the seasonal naÃ¯ve model (including prediction intervals)
2. Forecasts of the mean model (including prediction intervals)

```{r}
fc = fit %>%
  forecast(h = 7)

fc %>%
  filter(.model =="mean_m") %>%
  autoplot(scs_pedestrians)

fc %>%
  filter(.model =="snaive_m") %>%
  autoplot(scs_pedestrians)
```

#### 5.3 Residuals of the mean model

Analyze the residuals of the mean model. Specifically do the following

##### 5.3.1. Produce an autocorelation plot (a.k.a correlogram) of:

1. The original time series `Count` in `scs_pedestrians`.
2. The innovation residuals of the `mean` model.


```{r}
scs_pedestrians %>%
  ACF(Count, lag_max = 7*4) %>%
  autoplot()


fit %>%
  select(mean_m) %>%
  augment() %>%
  ACF(.innov, lag_max = 7*4) %>%
  autoplot()
```

Then compare both ACFs below

------

Both correlograms are the same, with high correlation in the lags that are multiples of 7, depicting a weekly seasonal pattern. In the Count ACF this is the expected behavior. However, in the mean model residuals it occurs because the mean model is not a good estimator for highly seasonal data since it produced flat fitted values and forecasts, without trend or seasonal components not explaining the underlying information and structure contained in seasonal or trend patterns. Therefore, with a mean model all this information, instead of being explained by the trend and seasonal components, it is captured by its residuals, which end up with very similar information as the initial data and therefore end in the same correlogram.

------

##### 5.3.2 Analyze whether the residuals of the mean model are biased or not

```{r}
fit %>%
  select(mean_m) %>%
  augment() %>% as_tibble() %>%
  summarise(mean = mean(.innov, na.rm = TRUE))
```

------

The mean of the residuals is very close to 0, which by definition means that they are unbiased and could lead us to think that this is a good model. However, we have already seen that this is a bad model for our specific data, but a bad model can also seem unbiased. The fact that the mean of the residuals is 0 does not mean that they are small, it just means that positive and negative residuals compensate.

------