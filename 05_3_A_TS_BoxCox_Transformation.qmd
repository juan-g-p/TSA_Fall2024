---
title: "05_3_A_TS_BoxCox_Transformation"
format: html
editor: source
params:
  print_sol_ex1: false
  print_sol_ex2: false
  hidden_notes: false
toc: true
toc-location: left
toc-depth: 6
self-contained: true
---

# Libraries

```{r}
library(fpp3)
```

```{r, include=FALSE, message=FALSE}
library(patchwork)
library(latex2exp)
```

# References

1.  Hyndman, R.J., & Athanasopoulos, G., 2021. Forecasting: principles and practice. 3rd edition.

# Note:

This notebook is not original material, but rather an expansioon of reference 1 (which is publicly available) with additional comments and explanations given by the professor to help students understand the concepts.

# Mathematical transformations

The transformations we are going to use in the context of time series are **useful**:

1.  If the data shows **variation that increases or decreases with the level of the series**. That is, if it is multiplicative to a certain extent.
2.  If the data is a simple exponential trend they can help linearize it (at the cost of interpretability).

Notation:

-   Original series: $y_1,\;...,y_t$.
-   Transformed series: $w_1,\;...,w_t$. \## log() transformations

## log() transformations

```{=tex}
\begin{align*}
  \boxed{w_t = log(y_t)}
\end{align*}
```
**NOTE:** the **log** function returns the **natural logarithm**. Natural logarithms (base $e$) are actually the reference for the rest of the logarithms. There are multiple reasons for this (beyond the scope of this course), the main one being that the derivative of $e^x$ is actually $e^x$. This has very important implications, particularly in the field of differential equations (general culture comment, beyond the scope of this course).

As a reminder of the properties of logarithms, **you should be comfortable enough with the basic properties of lograithms that you are able to always easily derive these two equations**. It is not necessary for this course but being able to prove those two statements would help you refresh your logarithm algebra (left for the student who wants to devote some time to this):

-   $a^x = e^{xLn(a)}$

-   $log_a(x) = \frac{log_b(x)}{log_b(a)}$

### Interpretation of logarithms

Changes in a log value are relative (or percentage changes) on the original scale. Example with log 10:

-   An increase of 1 on the $log_{10}$ scale corresonds to multiplying by 10 on the original scale.

We do not have time to see this in class and this should already have been clarified to you by other professors (particularly by those giving you Data Visualisation). I will upload a video to blackboard covering this.

### Transformation $log(1+x)$

**If the data contains either zeros or negative values, logarithms are not possible, since the real logarithm is only defined for** $x > 0$.

**If the data contains zeros and no negative values, then the transformation** $log(1+x)$ can be useful:

-   $log(1+x)$ can be useful for data with zeros that we feel would benefit from a log transformation. You can use the function `log1p()` directly for this:

**Question** can anybody tell me why this transformation is good when we have data with zeros? Specifically why $1$ and no other constant is chosen:

```{r, include=FALSE}
#| echo: false
#| warning: false
breaks = seq(-1, 10)
data <- tibble(
  x = seq(0.001, 10, 0.01),
  y = log(x),
  line_x = c(0.25, 1.75, rep(NA, 998)),
  line_y = c(-0.75, 0.75, rep(NA, 998))
)

p1 <- 
  ggplot(data) + 
  geom_line(aes(x=x, y=y)) +
  geom_line(aes(x=line_x, y=line_y), color="red", linetype="dashed") +
  geom_point(aes(x=1, y=0), color="red") +
  scale_x_continuous(breaks = seq(-5, 10),
                     minor_breaks = seq(-5, 10),
                     limits = c(-1, max(data$x)) # Set the x-axis limits
                     ) +
  ggtitle("y = log(x)") +
  geom_vline(xintercept = 0, linetype="dashed", 
             color = "blue", size=0.5)  +
  annotate("text", x = 1.8, y = -2, label = TeX(r"( $y \rightarrow -\infty$ \n if $x \rightarrow 0$)"), parse = TRUE) +
  geom_segment(aes(x = 1.2, y = -2.5, xend = 0.2, yend = -5), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), 
               color = "black", size=0.01) 
```

```{r, include=FALSE}
#| echo: false
#| warning: false
#| fig.width: 8
#| fig.height: 4
library(ggplot2)
library(tibble) # Make sure you load the tibble package
library(latex2exp)

breaks = seq(-1, 10, by = 1)
data <- tibble(
  x = seq(-0.999, 9, by = 0.01),
  y = log(1+x),
  line_x = c(-0.75, 0.75, rep(NA, length.out = 998)),
  line_y = c(-0.75, 0.75, rep(NA, length.out = 998))
)

p2 <- 
  ggplot(data) + 
  geom_line(aes(x=x, y=y)) +
  geom_line(aes(x=line_x, y=line_y), color="red", linetype="dashed") +
  geom_point(aes(x=0, y=0), color="red") +
  scale_x_continuous(
    breaks = breaks,
    minor_breaks = breaks,
    limits = c(-1, max(data$x)) # Set the x-axis limits
  ) +
  ggtitle("y = log(1+x)") +
  annotate("text", x = 0.5, y = -2, label = TeX(r"( $y \rightarrow -\infty$ \n if $x \rightarrow -1$)"), parse = TRUE) +
  geom_segment(aes(x = -0.2, y = -2.5, xend = -0.8, yend = -5), 
               arrow = arrow(type = "closed", length = unit(0.1, "inches")), 
               color = "black", size=0.01) + 
  
  geom_vline(xintercept = -1, linetype="dashed", 
                color = "blue", size=0.5)
```

```{r, fig.height=8, message=FALSE, warning=FALSE, echo=FALSE}
p1 / p2
```

```{r, include=params$hidden_notes, eval=FALSE}
1. The first advantage is that 0s in the original scale are also 0s in the
   transformed scale (if x = 0 then y = log(1+x) = 0)
   
2. The second advantage is that the function log(1+x) can be linearly 
   approximated by x in the viccinity of 0. See tangent lines in 
   the graph above
```

## Power transformations

```{=tex}
\begin{align*}
  w_t = y_t^p
\end{align*}
```
They are not as interpretable as the logarithmic transformation. They also tend to stabilize seasonal variations, as we saw in the example of the adiabatic anti-drug sales (see the introductory session to time series decomposition, where the concept of additive vs multiplicative schemes is introduced).

## Box-Cox transformations

This transformation includes both logarithms and power transformations. They depend on the parameter $\lambda$.

```{=tex}
\begin{equation}
  w_t  =
    \begin{cases}
      \log(y_t) & \text{if $\lambda=0$};  \\
      (\text{sign}(y_t)|y_t|^\lambda-1)/\lambda & \text{otherwise}.
    \end{cases}
\end{equation}
```
**Question:** what is the difference between a variable and a parameter?

```{r, eval=FALSE, include=params$hidden_notes}
In a function such as the Box-Cox transformation that has both parameters and 
variables:
  
* The variables (in this case y_t) define the domain of the function. In this
case the domain of the function is the range of possible values for y_t. The
function could have more than one variable.

* The parameters (in this case lambda) also take a range of values. Once we set
a specific  value for the parameters, we proceed to study the behavior of the
function: the possible values its variables (y_t in this case) may take and the
resulting values of the function.

Depending on the specific value of the parameters, the function behaves in 
one or other manner. That is, for each specific value of the parameters, we 
have one or other "version" or "behavior" of the function.

In the box-cox transformation, lambda is the parameter and y_t is the variable.
```

The Box-Cox transformation also attempts to stabilize the seasonal fluctuations and random variations. Specifically, \*\*it addresses cases where the variance of the time series grows with the level of the series\*

The following [link](https://otexts.com/fpp3/transformations.html#mathematical-transformations) leads to an interactive graph that clarifies the effect of $\lambda$ on the box-cox transformation in the context of time series.

-   The logarithm in a Box-Cox transformation is **always a natural logarithm**.
    -   $\lambda = 1$: no substantial transformation. Data is simply shifted downwards. ($w_t = y_t -1$)
    -   $\lambda = 0$: transformation equivalent to a natural logarithm.
    -   $\lambda = 1/2$: Square root + scaling transformation
    -   $\lambda = 1/3$: Cubic root + scaling transformation
    -   $\lambda = -1$: Inverse transformation (+ 1)
-   **Considerations when using this transformation**\`
    -   **If some data are zero or negative, use** $\lambda > 0$, since the real logarithm is not defined in this case.
    -   Always check your results depicting the time series.
    -   A **low value of** $\lambda$ can result in extremely large prediction intervals.
    -   **If the best value of** $\lambda$ is very close to 0, it is *preferable to use a log transformation* because it has *much better interpretability*.
    -   The **final judge of the transformation** will be the **performance of the model you are trying to use** and the **uncertainty associated to its predicions** (prediction intervals). This is very important. Throughout the course we will learn how to evaluate the performance of our models.

### Caveat of Box-Plot like transformations

Power transformations (such as the box-plot transformation) **require the variability of the series to vary proportionally to the level of the series**.

Let us look at data about the production of gas in Canada:

```{r}
canadian_gas %>%
  autoplot(Volume) +
  labs(
    x = "Year", y = "Gas production (billion cubic meters)",
    title = "Monthly Canadian gas production"
  )
```

```{r}

```

-   The **variation of the series is not proportional to the amount of gas produced in Canada.**
    -   **For small and large production levels:** small variations in the seasonal pattern
    -   **Moderate production leveks:** between 1975 and 1990. The variation in the seasonal pattern is large.

In this case the box-cox transformation would not be of much help to render the variance of the seasonal component independnet from the level of the series.

### Guerrero feautre to pick a variable for the parameter lambda

The Guerrero feature suggest a value of lambda that results in the best Box-Cox transformation to make the scheme of the series additive. Like everything It does not work 100%, but for most cases it will result in a good value to render the scheme of the series additive.

Let us look at the gas production in Australia:

```{r}
aus_production %>% 
  autoplot(Gas) +
  scale_x_yearquarter(
    breaks = "10 years",
    minor_breaks = "1 year"
  )
```

In the example below we apply it to the `Gas` time series within the dataset `aus_production`, loaded along with `fpp3`.

```{r}
lambda <- aus_production %>%
  features(Gas, features = guerrero) %>%
  pull(lambda_guerrero)

lambda
```

The suggested value of lambda is 0.1, which is neither too close to 0 (log) or 1 (no transformation). So in principle we will apply the box-cox transformation.

```{r}
aus_production %>%
  autoplot(box_cox(Gas, lambda)) +
  scale_x_yearquarter(
    breaks = "10 years",
    minor_breaks = "1 year"
  )
```

### Usual wrokflow with the box-cox tranformation

If you decide that a box-cox transformation could be of help for the case at hand, the flowchart below is a general outline of the process:

![](figs/box_cox_flowchart.png){width="80%" fig.align="center"}

1.  Check the value of lambda suggested by the `guerrero` feature.
2.  If the value of lambda is very close to 1, do not apply a transformation, since the Box-Cox transformation will barely have a noticeable effect yet do away with the interpretability of the variable.
3.  If the value of lambda is very close to 0, apply a $log()$ transformation instead, since their effect will be very close and the log transformation preserves the interpretability.
4.  If the value of lambda is neither close to 0, nor close to 1, apply the Box-cox transformation.

In the end, **always check the result creating a timeplot of the data**. You may also add some quantitative metrics as the moving standard deviation (not done in this course) to help you understand if the variance increases with the level of the series after the transforamtions or if it has been stabilized.

The exercises and homework at the end of the notebook will bring clarity to this process.

## Advanced - other transformations for data with 0s or negative values

The following [article](https://robjhyndman.com/hyndsight/transformations/) is worth reading. This is beyond the scope of the course, but it is a good reference if you ever face this issue.

## Some notes on transformations

-   **Often no transformation is needed** and we are able to produce a good forecast with the original data. **Do not transform just for the sake of transforming**, the end goal is to **make data more palatable for a model so that it improves its performance**.
    -   The **final judge of the transformation** will be the **performance of the model you are trying to use** and the **uncertainty associated to its predicions** (prediction intervals). This is very important. Throughout the course we will learn how to evaluate the performance of our models.
-   Transformations that have good interpretability (such as log transformations) are preferred if they work well enough.
-   Transformations **can have a very large effect on Prediction Intervals**.
-   Transformations **must be reversed to obtain forecasts on the original scale**.

# Examples

For each of the following series, make a graph of the data. If transforming seems appropriate, do so and describe the effect.

-   United states GDP from `global economy`
-   Slaughter of Victorian "Bulls, bullocks and steers" in `aus_livestock`
-   Victorian Electricity Demand from `vic_elec`
-   Tobacco from aus_production
-   The following retail series:

```{r}
retail_series <- aus_retail %>%
  filter(`Series ID` == "A3349767W")
```

```{r}

```

### Example 1. United states gdp from global economy:

```{r}
us_economy <- global_economy %>%
  filter(Country == "United States")
us_economy %>%
  autoplot(GDP)
```

In the graph above, the derivative increases with the level of the series (the derivative is the slope of the tangent tangent to the graph at each point). This is **another form of increasing variance with the level of the series**, a non-linear trend. The box-cox transformation is also useful to address this.

```{r, include=params$hidden_notes}
# The trend is non-linear. Let us attempt to render it linear with a transformation
us_economy %>%
  autoplot(box_cox(GDP, 0))
```

```{r, include=params$hidden_notes, eval = FALSE}
A log transformation (Box plot with $\lambda = 0$) appears too strong.
Let's see what guerrero's method suggests:
```

```{r, include=params$hidden_notes}
lambda <- us_economy %>%
  features(GDP, features = guerrero) %>%
  pull(lambda_guerrero)

lambda
```

```{r, include=params$hidden_notes}
us_economy %>%
  autoplot(box_cox(GDP, lambda))
```

```{r, include=params$hidden_notes, eval = FALSE}
The result is much more linear. 

Yet we hav los all the interpretability of the respone variable. We no longer
have a simple, interpretable relationship between the transformed variable and
the original variable.

Also, transformations can have a substantial effect on the prediction intervals.

To judge if the transformation is worth it, we could compare models fitted to the
original data and models fitted to the transformed data using cross-validation.
We will study this throughout the course.
```

### Example 2. Slaughter of Victorian "Bulls, bullocks and steers"

```{r}
vic_bulls <- aus_livestock %>%
  filter(State == "Victoria", Animal == "Bulls, bullocks and steers")
vic_bulls %>%
  autoplot(Count)
```

```{r}

```

```{r, include=params$hidden_notes, eval=FALSE}
The variation of the series appears to slightly vary with the numbers of bulls
slaughtered. A transformation could be useful.

Let us see what the guerrero feature suggests:
```

```{r, include=params$hidden_notes}
vic_bulls %>%
  features(Count, features = guerrero)
```

```{r, include=params$hidden_notes, eval=FALSE}
The value of lambda suggestes is so small that it is best to perform a log
transformation (which would correspond to lambda = 0) that at least preserves
the interpretability of the data.
```

```{r, include=params$hidden_notes}
vic_bulls %>%
  autoplot(log(Count))
```

```{r, include=params$hidden_notes, eval=FALSE}
A log transformation (lambda = 0) is pretty close and has much better interpretability.
```

### Example 3. Tobacco from aus_production

```{r}
aus_production %>%
  autoplot(Tobacco)
```

```{r, include=params$hidden_notes, eval=FALSE}
This variation in this series appears to be mostly constant
across different levels of the series.
Lets look at the guerrero feature.
```

```{r, include=params$hidden_notes}
lambda <- aus_production %>%
  features(Tobacco, features = guerrero) %>%
  pull(lambda_guerrero)

lambda
```

```{r, include=params$hidden_notes}
# The value appears to be very close to 1.
aus_production %>%
  autoplot(box_cox(Tobacco, lambda))
```

```{r, include=params$hidden_notes, eval=FALSE}
Would you perform this transformation?

No. The value of lambda is very close to 1 (no transformation). 
There is no substantial gain and we completely lose the interpretability
of the response variable.
```

### Example 4. Retail series

```{r}
retail_series <- aus_retail %>%
  filter(`Series ID` == "A3349767W")

autoplot(retail_series)
```

```{r, include=params$hidden_notes, eval=FALSE}
The variation appears to be proportional to the level of the series.

Lets inspect the guerrero feature
```

```{r, include=params$hidden_notes}
retail_series %>%
  features(Turnover, features = guerrero)
```

```{r, include=params$hidden_notes, eval=FALSE}
**Question** given this value of lambda, what would you do?

Since the value is so close to 0, the sensible thing is to perform a logarithmic
transformation, which is much more interpretable.
```

```{r, include=params$hidden_notes}
retail_series %>%
  autoplot(log(Turnover)) + 
  labs(
    title = retail_series$Industry[1],
    subtitle = retail_series$State[1]
  )
```

# Homework

**NOTE:** For these exercises you need separate datasets that will be uploaded to blackboard. You need the library `readr` as well to read csv files with the function \`read_csv\`\`

Judge if one of the mathematical transformations explained in class is sensible or not.

If so, specify which transformation you have chosen and provide adequate justification

```{r, include=FALSE, eval=FALSE}
# Install these packages if you do not have them installed 
install.packages("devtools") 
devtools::install_github("FinYang/tsdl") # Bank of time-series data`
```

```{r, include=FALSE, message=FALSE, warning=FALSE}
library(tsdl)
```

## Exercise 1 - Australian imports from Japan

The code below imports the data contained in the file *australian_imports_japan.csv*, which is located in the folder *ZZ_Datasets* in google drive. When you run the code, **a pop-up window will prompt you to select the .csv file** and import the data.

The following is a plot of the data.

```{r, include=FALSE}
aus_import_japan <- as_tsibble(subset(tsdl, "Macroeconomic", 12)[[3]])
```

```{r, eval=FALSE}
aus_import_japan <- 
  readr::read_csv(file.choose()) %>% 
  mutate(index=yearmonth(index)) %>% 
  as_tsibble()

aus_import_japan %>% 
  autoplot() +
  labs(x = "year-month",
       y = "Australian imports from Japan (thousands $)")
```

```{r, echo=FALSE}
aus_import_japan %>% 
  autoplot() +
  labs(x = "year-month",
       y = "Australian imports from Japan (thousands $)")  
```

```{r, include=params$print_sol_ex1}
aus_import_japan %>%
  features(value, features = guerrero)
```

```{r, include=params$print_sol_ex1, eval=FALSE}
The amplitude of the seasonal component appears to be proportional to the level
of the series

Guerreros feature (lambda) is very close to 0, so in this case we will opt for
a log transformation because it is much more interpretable
```

```{r, include=params$print_sol_ex1}
aus_import_japan %>%
  mutate(log_imports = log(value)) %>%
  autoplot(log_imports)
```

## Exercise 2 - Private housing units started - USA

The code below imports the data contained in the file *private_housing_US.csv*, which is located in the folder *ZZ_Datasets* in google drive. When you run the code, **a pop-up window will prompt you to select the .csv file** and import the data.

The following is a plot of the data.

```{r, include=FALSE}
private_housing_Us <- as_tsibble(subset(tsdl, "Microeconomic", 12)[[2]])
```

```{r, eval=FALSE}
private_housing_Us <- 
  readr::read_csv(file.choose()) %>% 
  mutate(index=yearmonth(index)) %>% 
  as_tsibble()

autoplot(private_housing_Us) +
labs(x = "year-month",
     y = "Private housing units started, USA: monthly")  
```

```{r, echo=FALSE}
autoplot(private_housing_Us) +
labs(x = "year-month",
     y = "Private housing units started, USA: monthly")  
```

```{r, include=params$print_sol_ex2, eval=FALSE}
Data does not appear to vary with the level of the series
It appears that there are different seasonal periods in the time series...
These issues are probably solved better with modeling rather than transformations.

This is outside the question asked by the exercise, but let us attempt to detrend
the time series and produce an ACF of the detrended series. I will use the trend
produced by STL to detrend the series. The trend estimate is produce using LOESS
regression and should be a good estimate.

After removing the effect of the trend, we may see some relevant seasonal periods
```

```{r, include=params$print_sol_ex2}
stl_trend <- 
  private_housing_Us %>% 
  model(STL(value)) %>% # Only interested in the trend
  components() %>% 
  pull(trend) # Extract the column trend as a vector and stre it in stl_trend

# Attempt to detrend the time series. Attempt to do it in an additive manner
private_housing_Us <- 
  private_housing_Us %>% 
    mutate(
      detrended = value - stl_trend
    )

# Looks like this has successfully detrended the series
private_housing_Us %>% autoplot(detrended)

# ACF of the detrended component
private_housing_Us %>% ACF(detrended, lag_max = 36) %>% autoplot()
```
```{r, include=params$print_sol_ex2, eval=FALSE}
The seasonal pattern is not anymore so obvious, it is not a "textbook" casse. Yet
we can definately see some yearly seasonality in the spike of the ACF at lag 12.

In the second part of the course, we will study how to characterize to some extent the
seasonal strength (using STL features).

The series itself does not seem to have much of an increasing variance with the
level of the series, which is what the box-cox tranformation addresses. But in 
any case, let us inspect the suggested transformation by the guerrero feature
to see what results from it:
```


```{r, include=params$print_sol_ex2}
lambda <- private_housing_Us %>%
  features(value, features = guerrero) %>%
  pull(lambda_guerrero)

# Neither close to 0 nor to 1.
lambda
```

```{r, include=params$print_sol_ex2}
private_housing_Us %>%
  autoplot(box_cox(value, lambda))
```

```{r, include=params$print_sol_ex2, eval=FALSE}
Visual inspection does not reveal a noticeable effect of the transformation.

Yet, all the interpretability of the variable is lost. In this case, 
the box-cox transformation does not seem to be of help.

This is a REMINDER of the importance of assessing the effect of the suggested
transformation and not just applying it blindly.
```
